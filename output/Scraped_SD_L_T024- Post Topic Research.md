# Scraped Content

Total URLs: 24

---

## âœ“ Google's FACTS Benchmark Exposes AI Factuality Ceiling at 70% | Enrique L. posted on the topic | LinkedIn

**URL:** https://www.linkedin.com/posts/enrique-l-72617b32_the-70-factuality-ceiling-why-googles-activity-7405255121938300928-LJ5E

Enrique L.

1mo

Report this post

ðŸ¤¯ The AI Factuality Crisis is Real: Google Just Proved It.

70%. Thatâ€™s the factuality ceiling exposed by Googleâ€™s new FACTS benchmark.

For too long, we've celebrated generative AI benchmarks focused on speed, coding, and basic instruction following. But when it comes to enterprise adoption, trust isn't a featureâ€”it's a requirement.

If the foundation of your mission-critical AI applicationsâ€”from agentic web browsing to complex tool useâ€”can only guarantee 70% accuracy, you're building on quicksand. The remaining 30% isn't just a slight error; it's a catastrophic business risk.

This is the wake-up call for Enterprise AI leaders.

In this article, we break down:

â€¢ What Googleâ€™s new FACTS benchmark specifically measures.

â€¢ Why the 70% ceiling changes the math on your ROI calculations.

â€¢ The immediate steps your organization must take to mitigate this risk.

There's no shortage of benchmarks, but there is a shocking shortage of reliable factuality.

Don't let your AI strategy be defined by a coin flip. Tap the link to read the full analysis on why the 70% barrier must be addressed immediately.

https://lnkd.in/eQ9z2-bQ

#GenerativeAI

#EnterpriseAI

#LLMs

#AIStrategy

#Factuality

#GoogleFACTS

#DigitalTransformation

#TechNews

1

Like

Comment

Share

Copy

LinkedIn

Facebook

X

To view or add a comment,

sign in

---

## âœ— https://medium.com/@brian22carpenter/the-illusion-of-100-how-one-conversation-broke-the-accuracy-myth-in-ai-deb8ebb34125

**URL:** https://medium.com/@brian22carpenter/the-illusion-of-100-how-one-conversation-broke-the-accuracy-myth-in-ai-deb8ebb34125

Error scraping: 403 Client Error: Forbidden for url: https://medium.com/@brian22carpenter/the-illusion-of-100-how-one-conversation-broke-the-accuracy-myth-in-ai-deb8ebb34125

---

## âœ— https://medium.com/enrique-dans/when-ai-gets-it-wrong-why-responsibility-still-lies-with-us-d6d1890f395d

**URL:** https://medium.com/enrique-dans/when-ai-gets-it-wrong-why-responsibility-still-lies-with-us-d6d1890f395d

Error scraping: 403 Client Error: Forbidden for url: https://medium.com/enrique-dans/when-ai-gets-it-wrong-why-responsibility-still-lies-with-us-d6d1890f395d

---

## âœ“ [2305.17493] The Curse of Recursion: Training on Generated Data Makes Models Forget

**URL:** https://arxiv.org/abs/2305.17493

Computer Science > Machine Learning

arXiv:2305.17493

(cs)

[Submitted on 27 May 2023 (

v1

), last revised 14 Apr 2024 (this version, v3)]

Title:

The Curse of Recursion: Training on Generated Data Makes Models Forget

Authors:

Ilia Shumailov

,

Zakhar Shumaylov

,

Yiren Zhao

,

Yarin Gal

,

Nicolas Papernot

,

Ross Anderson

View a PDF of the paper titled The Curse of Recursion: Training on Generated Data Makes Models Forget, by Ilia Shumailov and 5 other authors

View PDF

HTML (experimental)

Abstract:

Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of content generated by LLMs in data crawled from the Internet.

Comments:

Fixed typos in eqn 4,5

Subjects:

Machine Learning (cs.LG)

; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Cryptography and Security (cs.CR); Computer Vision and Pattern Recognition (cs.CV)

Cite as:

arXiv:2305.17493

[cs.LG]

(or

arXiv:2305.17493v3

[cs.LG]

for this version)

https://doi.org/10.48550/arXiv.2305.17493

Focus to learn more

arXiv-issued DOI via DataCite

Submission history

From: Zakhar Shumaylov [

view email

]

[v1]

Sat, 27 May 2023 15:10:41 UTC (1,773 KB)

[v2]

Wed, 31 May 2023 10:39:26 UTC (1,847 KB)

[v3]

Sun, 14 Apr 2024 05:20:10 UTC (1,851 KB)

Full-text links:

Access Paper:

View a PDF of the paper titled The Curse of Recursion: Training on Generated Data Makes Models Forget, by Ilia Shumailov and 5 other authors

View PDF

HTML (experimental)

TeX Source

view license

Current browse context:

cs.LG

<Â prev

|

nextÂ >

new

|

recent

|

2023-05

Change to browse by:

cs

cs.AI

cs.CL

cs.CR

cs.CV

References & Citations

NASA ADS

Google Scholar

Semantic Scholar

3 blog links

(

what is this?

)

export BibTeX citation

Loading...

BibTeX formatted citation

Ã—

loading...

Data provided by:

Bookmark

Bibliographic Tools

Bibliographic and Citation Tools

Bibliographic Explorer Toggle

Bibliographic Explorer

(

What is the Explorer?

)

Connected Papers Toggle

Connected Papers

(

What is Connected Papers?

)

Litmaps Toggle

Litmaps

(

What is Litmaps?

)

scite.ai Toggle

scite Smart Citations

(

What are Smart Citations?

)

Code, Data, Media

Code, Data and Media Associated with this Article

alphaXiv Toggle

alphaXiv

(

What is alphaXiv?

)

Links to Code Toggle

CatalyzeX Code Finder for Papers

(

What is CatalyzeX?

)

DagsHub Toggle

DagsHub

(

What is DagsHub?

)

GotitPub Toggle

Gotit.pub

(

What is GotitPub?

)

Huggingface Toggle

Hugging Face

(

What is Huggingface?

)

Links to Code Toggle

Papers with Code

(

What is Papers with Code?

)

ScienceCast Toggle

ScienceCast

(

What is ScienceCast?

)

Demos

Demos

Replicate Toggle

Replicate

(

What is Replicate?

)

Spaces Toggle

Hugging Face Spaces

(

What is Spaces?

)

Spaces Toggle

TXYZ.AI

(

What is TXYZ.AI?

)

Related Papers

Recommenders and Search Tools

Link to Influence Flower

Influence Flower

(

What are Influence Flowers?

)

Core recommender toggle

CORE Recommender

(

What is CORE?

)

IArxiv recommender toggle

IArxiv Recommender

(

What is IArxiv?

)

Author

Venue

Institution

Topic

About arXivLabs

arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community?

Learn more about arXivLabs

.

Which authors of this paper are endorsers?

|

Disable MathJax

(

What is MathJax?

)

---

## âœ— https://medium.com/@siddharthc96/understanding-ai-hallucinations-why-they-happen-and-how-to-address-them-c1249c07601d

**URL:** https://medium.com/@siddharthc96/understanding-ai-hallucinations-why-they-happen-and-how-to-address-them-c1249c07601d

Error scraping: 403 Client Error: Forbidden for url: https://medium.com/@siddharthc96/understanding-ai-hallucinations-why-they-happen-and-how-to-address-them-c1249c07601d

---

## âœ— https://www.producthunt.com/p/general/authenticity-and-trust-issues-in-ai-generated-content?utm_source=chatgpt.com

**URL:** https://www.producthunt.com/p/general/authenticity-and-trust-issues-in-ai-generated-content?utm_source=chatgpt.com

Error scraping: 403 Client Error: Forbidden for url: https://www.producthunt.com/p/general/authenticity-and-trust-issues-in-ai-generated-content?utm_source=chatgpt.com

---

## âœ“ The Next Generation of AI Agents: Large Action Models Explained - Gradient Flow

**URL:** https://gradientflow.com/large-action-models-explained/

As AI agents become commonplace in enterprise workflows, teams are discovering the limitations of building task-specific automated systems from scratch.

Large Action Models (LAMs)

represent the foundational layer that transforms how we build agentsâ€”providing the general-purpose perception, planning, and execution capabilities that individual agents can leverage rather than reinvent. Instead of building isolated automation tools, LAMs enable teams to create more capable, adaptable agentic systems that can operate across diverse contexts and applications.

LAMs

represent a shift in AI from passive content generation to active task execution. Unlike

Large Language Models

(LLMs) that excel at text generation and understanding, or

Visual Language Models

(VLMs) that combine text and visual processing, LAMs are designed to autonomously perceive, plan, and execute multi-step actions within digital and physical environments. While AI

agents

are able to perform specific automated tasks,

LAMs serve as foundational architecture

that enable

more general-purpose

, language-driven agentic systems capable of operating across diverse contexts and applications.

Support our work by becoming a paid subscriber.

The core distinction lies in their operational approach. While LLMs can describe flight booking and VLMs can analyze booking screenshots, LAMs actually navigate websites and complete reservations.

That said, current implementations work best in controlled environments.

Many LAMs achieve this capability by pairing neural perception modules with symbolic planners in a neuroâ€‘symbolic architecture, though some

recent systems rely on a single endâ€‘toâ€‘end

neural network instead.

Recent developments have validated this potential.

OpenAIâ€™s ChatGPT agent

, launched in July 2025, represents the first major production deployment of a unified LAM system. By combining web browsing capabilities, deep research functionality, and terminal access within a single model, ChatGPT agent demonstrates how LAMs can move beyond controlled environments to handle complex, multi-step workflows across diverse applications. The system achieved state-of-the-art performance on benchmarks like Humanityâ€™s Last Exam (41.6% accuracy) and FrontierMath (27.4% accuracy), while maintaining the safety controls necessary for enterprise deployment.

In the case of

ChatGPT agent

the underlying large action model is not separately exposed; OpenAI surfaces it as a managed service with safety guardrails. Purists might say the â€œLAMâ€ is the model inside the service, while â€œChatGPT Agentâ€ is a LAM-powered agent.

A Spectrum of LAM Use Cases

Large Action Models are transitioning from concept to reality, tackling complex, multi-step sequences of actions once exclusively performed by humans. In the consumer sphere, this technology is emerging in mobile integrations like

Google Gemini Live

, which organize personal data across applications, and personal assistants like the

Motorola LAM

or

Rabbit R1

, which handle tasks like ordering meals or booking rides.

However, early implementations show mixed real-world results.

This same power is being applied to streamline business operations. Within the enterprise, ServiceNow agents automate internal IT and HR workflows, while specialized tools like

11xâ€™s â€œAliceâ€

execute external-facing tasks like prospect research and sales outreach.

Similarly, specialized agents like

Shortcut

are emerging to automate complex knowledge work within specific applications, such as performing multi-step data modeling and analysis in Microsoft Excel.

The release of

ChatGPT agent

marks a significant milestone in LAM maturity, offering the first widely-available unified system that consolidates multiple capabilities. Unlike earlier specialized tools, ChatGPT agent integrates visual web browsing, text-based research, terminal access, and API connectivity within a single model. This architectural approach enables seamless transitions between different interaction modesâ€”gathering calendar information through an API, analyzing web content via text processing, and completing transactions through visual interface manipulation.

For development teams, this represents a shift from integrating multiple specialized agents to leveraging a foundational LAM that can adapt its approach based on task requirements. The systemâ€™s ability to generate editable artifacts (presentations, spreadsheets, code) while maintaining context across tool switches demonstrates the practical value of unified LAM architectures over tool-chaining approaches.

(

click to enlarge

)

The application of LAMs extends into highly specialized and regulated fields. In software engineering, AI developers like

Cognition Devin

attempt to independently write, test, and debug code, while frameworks like

Microsoft AutoDev

coordinate teams of agents on complex programming projects. In data-intensive sectors such as healthcare and finance, these models reduce a

[Content truncated...]

---

## âœ— https://medium.com/illumination/painful-ai-chatbots-are-only-right-69-of-the-time-9e587f330dff

**URL:** https://medium.com/illumination/painful-ai-chatbots-are-only-right-69-of-the-time-9e587f330dff

Error scraping: 403 Client Error: Forbidden for url: https://medium.com/illumination/painful-ai-chatbots-are-only-right-69-of-the-time-9e587f330dff

---

## âœ“ Success Online Is Changing: What Actually Matters When Everything Can Be Automated - Indie Hackers

**URL:** https://www.indiehackers.com/post/success-online-is-changing-what-actually-matters-when-everything-can-be-automated-0c9f988ec1?utm_source=chatgpt.com

Success Online Is Changing: What Actually Matters When Everything Can Be Automated

For indie founders, the internet used to feel like a place where effort showed.  You wrote the posts yourself, shipped the updates yourself, and built an audience one real interaction at a time.

But that landscape is shifting fast.

Today, anyone can spin up content at scale. Long threads, polished posts, value-packed breakdowns - all produced instantly, and when everyone can generate â€œsignal,â€ it becomes harder to tell whatâ€™s signal and whatâ€™s noise.

That puts builders in a strange spot:

If content, engagement, and even â€œpersonal brandingâ€ can be automated, then what does meaningful success look like now?

Itâ€™s a question thatâ€™s becoming more relevant by the week.

The Metrics We Chased Donâ€™t Mean What They Used To

Every builder has looked at their analytics and asked, â€œIs this actually telling me anything real?â€

Traffic spikes can come from luck, follower counts can be engineered and engagement tricks work until they donâ€™t. AI can make anyone look productive.

Weâ€™re hitting a saturation point where a polished online presence is no longer proof of momentum - just proof of tools.

The usual metrics still matter, but they donâ€™t necessarily reflect

depth of connection

,

user trust

, or

what you actually learned while building

.

In other words: the numbers tell part of the story, but not the important part.

Authenticity Has Become a Competitive Advantage

With so much automated content floating around, people are gravitating toward founders who still talk like humans.

Not hyper-polished, not overly strategic, just straightforward and real.

When you share lessons that come from your own build journey - the painful parts, the weird detours, the small wins - people feel that, and it cuts through the algorithmic noise instantly.

Thereâ€™s no shortcut for lived experience, uou canâ€™t outsource your mistakes or your insights and those are the things other builders actually care about.

The maker community is good at detecting authenticity. Sometimes painfully so.

Writing Is Still One of the Only Places You Canâ€™t Fully Fake

You can automate visuals, landing pages, even product demos, but writing - the kind where youâ€™re processing your decisions, explaining your thinking, or documenting your journey - still feels human in a way most tools canâ€™t imitate.

When you write about your product, or about the thing you learned last week, your mindset shows up. Your assumptions show up and your blind spots show up too.

And ironically, thatâ€™s what makes it valuable.

Real writing has bugs just like code and sometimes those â€œbugsâ€ are the reason people trust you.

AI Can Generate Content, But It Canâ€™t Build Your Perspective

The difference between an AI-generated thread and a founderâ€™s reflection is the same difference between documentation and experience.

One tells you what something

should

look like.

The other tells you how it actually went.

That gap - the lived part - is where all the meaningful insight sits.

AI can produce accurate summaries, it can rewrite your ideas in a cleaner tone, even can infer patterns.

But it canâ€™t ship your product for you, it canâ€™t feel your uncertainty when you launch.  It canâ€™t sit through customer interviews that go sideways, neither replace the personal cost of building something from scratch.

Your perspective is the only IP AI canâ€™t clone.

Real Progress Often Looks Quiet From the Outside

A lot of the wins builders care about never show up on a public feed:

Shipping after procrastinating for weeks

Finally understanding a userâ€™s pain point

Fixing a bug that drained your energy

Choosing long-term clarity over short-term hacky metrics

Writing a postmortem that actually hurts a little

These moments wonâ€™t go viral.

But they move your product forward more than any engagement hack. And writing them down - even privately - helps you capture what actually changed.

A New Definition of Success for Makers

For indie hackers, success is shifting toward something more sustainable:

Not just publishing often, but publishing honestly

Not just audience growth, but audience trust

Not just traction, but traction built on real value

Not just visibility, but alignment with why youâ€™re building

This is the kind of success you canâ€™t automate.  It comes from reflection, iteration, and the messy parts of building in public.

And when you share that side of your work, people notice - because itâ€™s rare.

The Simple Advantage We Still Have: Being Human

Automation is only going to grow.

Content will multiply.

Everything will speed up.

But people still respond to the same old things:

Real stories, real curiosity, real lessons learned the hard way and real builders trying to figure it out in public.

Thatâ€™s not something the tools can replace.

And maybe thatâ€™s the only metric that still matters:

whether your work carries something only you could have written.

Photo by

Michael Hutter

on

Unsplash

---

## âœ“ Knowledge graph accuracy evaluation: an LLM-enhanced embedding approach | International Journal of Data Science and Analytics | Springer Nature Link

**URL:** https://link.springer.com/article/10.1007/s41060-024-00661-3

Knowledge graph accuracy evaluation: an LLM-enhanced embedding approach

Regular Paper

Published:

08 October 2024

VolumeÂ 20

,Â pages 3021â€“3035, (

2025

)

Cite this article

Save article

View saved research

International Journal of Data Science and Analytics

Aims and scope

Submit manuscript

Abstract

As an effective way for knowledge representation and knowledge storage, knowledge graph has been widely used in various fields. However, with the rapid increase of scale and volume of various knowledge graphs, there will inevitably be some knowledge quality matters. To evaluate the accuracy of knowledge graph effectively and efficiently, a common paradigm is to match the facts in knowledge graph with specific external knowledge. In this study, an LLM-enhanced (large language model enhanced) embedding framework is designed, integrating the verification ability of large language models to further evaluate the embedding results. First an optimized embedding model is proposed to make use of knowledge graphâ€™s internal structural information to measure whether the relation of a given triplet is probably founded. Then, the triplets which have less paths to support themselves are selected as the questionable ones, as their correctness cannot be determined confidently. Finally, the questionable triplets are filtered, and LLMs are adopted for further fact verification as external knowledge. The above three parts are aggregated to achieve the automated, accurate and efficient evaluation for knowledge graphs.

This is a preview of subscription content,

log in via an institution

to check access.

Access this article

Log in via an institution

Subscribe and save

Springer+

from â‚¬37.37 /Month

Starting from 10 chapters or articles per month

Access and download chapters and articles from more than 300k books and 2,500 journals

Cancel anytime

View plans

Buy Now

Buy article PDF 39,95 â‚¬

Price includes VAT (India)

Instant access to the full article PDF.

Institutional subscriptions

Fig. 1

Fig. 2

Fig. 3

Fig. 4

Fig. 5

Similar content being viewed by others

Unlocking the Power of LLM-Based Question Answering Systems: Enhancing Reasoning, Insight, and Automation with Knowledge Graphs

Chapter

Â© 2024

Incorporating Text into the Triple Context for Knowledge Graph Embedding

Chapter

Â© 2018

Exploring the Generalization of Knowledge Graph Embedding

Chapter

Â© 2020

Explore related subjects

Discover the latest articles, books and news in related subjects, suggested using machine learning.

Expertise

Ontology

Graphemics

Knowledge Management

Knowledge Based Systems

Linear Logic

Notes

In embedding models, the number of training set triplets are constrained by BFS depth and subgraph size.

References

Saxena, A., Tripathi, A., Talukdar, P.: Improving multi-hop question answering over knowledge graphs using knowledge base embeddings. In: Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (2020)

Hildebrandt, M., QuinteroÂ Serna, J.A., Ma, Y., Ringsquandl, M., Joblin, M., Tresp, V.: Reasoning on knowledge graphs with debate dynamics. In: Proceedings of the AAAI Conference on Artificial Intelligence, pp. 4123â€“4131 (2020)

Dong, X.L., Gabrilovich, E., Heitz, G., Horn, W., Murphy, K.P., Sun, S., Zhang, W.: From data fusion to knowledge fusion. Proc. VLDB Endow.

7

, 881â€“892 (2014)

Article

Google Scholar

Wang, R.Y., Strong, D.M.: Beyond accuracy: what data quality means to data consumers. J. Manag. Inf. Syst.

12

, 5â€“33 (1996)

Article

Google Scholar

Gao, J., Li, X., Xu, Y.E., Sisman, B., Dong, X.L., Yang, J.: Efficient knowledge graph accuracy evaluation. Proc. VLDB Endow.

12

, 1679â€“1691 (2019)

Article

Google Scholar

Suchanek, F.M., Kasneci, G., Weikum, G.: Yago: A core of semantic knowledge unifying wordnet and Wikipedia. In: Proceedings of the 16th International Conference on World Wide Web (2007)

Qi, Y., Zheng, W., Hong, L., Zou, L.: Evaluating knowledge graph accuracy powered by optimized humanâ€“machine collaboration. In: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. KDDâ€™22, pp. 1368â€“1378. Association for Computing Machinery, New York, NY, USA (2022)

Ojha, P., Talukdar, P.: KGEval: accuracy estimation of automatically constructed knowledge graphs. In: Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (2017)

Amaral, G., Rodrigues, O., Simperl, E.P.B.: Prove: a pipeline for automated provenance verification of knowledge graphs against textual sources (2022).

arXiv:2210.14846

Liu, S., dâ€™Aquin, M., Motta, E.: Measuring accuracy of triples in knowledge graphs. In: International Conference on Language, Data, and Knowledge (2017)

Jia, S., Xiang, Y., Chen, X., Wang, K.: Triple trustworthiness measurement for knowledge graph. In: The World Wide Web Conference (2019)

Zhu, Y., Wang, X., Chen, J., Qiao, S., Ou, Y., Yao, Y., Deng, S., Chen, H., Zhang, N.: LLMS for knowledge graph constructi

[Content truncated...]

---

## âœ“ [2305.14251] FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation

**URL:** https://arxiv.org/abs/2305.14251

Computer Science > Computation and Language

arXiv:2305.14251

(cs)

[Submitted on 23 May 2023 (

v1

), last revised 11 Oct 2023 (this version, v2)]

Title:

FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation

Authors:

Sewon Min

,

Kalpesh Krishna

,

Xinxi Lyu

,

Mike Lewis

,

Wen-tau Yih

,

Pang Wei Koh

,

Mohit Iyyer

,

Luke Zettlemoyer

,

Hannaneh Hajishirzi

View a PDF of the paper titled FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation, by Sewon Min and 8 other authors

View PDF

Abstract:

Evaluating the factuality of long-form text generated by large language models (LMs) is non-trivial because (1) generations often contain a mixture of supported and unsupported pieces of information, making binary judgments of quality inadequate, and (2) human evaluation is time-consuming and costly. In this paper, we introduce FACTSCORE, a new evaluation that breaks a generation into a series of atomic facts and computes the percentage of atomic facts supported by a reliable knowledge source. We conduct an extensive human evaluation to obtain FACTSCOREs of people biographies generated by several state-of-the-art commercial LMs -- InstructGPT, ChatGPT, and the retrieval-augmented PerplexityAI -- and report new analysis demonstrating the need for such a fine-grained score (e.g., ChatGPT only achieves 58%). Since human evaluation is costly, we also introduce an automated model that estimates FACTSCORE using retrieval and a strong language model, with less than a 2% error rate. Finally, we use this automated metric to evaluate 6,500 generations from a new set of 13 recent LMs that would have cost $26K if evaluated by humans, with various findings: GPT-4 and ChatGPT are more factual than public models, and Vicuna and Alpaca are some of the best public models. FACTSCORE is available for public use via `pip install factscore`.

Comments:

25 pages; 7 figures. Published as a main conference paper at EMNLP 2023. Code available at

this https URL

Subjects:

Computation and Language (cs.CL)

; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)

Cite as:

arXiv:2305.14251

[cs.CL]

(or

arXiv:2305.14251v2

[cs.CL]

for this version)

https://doi.org/10.48550/arXiv.2305.14251

Focus to learn more

arXiv-issued DOI via DataCite

Submission history

From: Sewon Min [

view email

]

[v1]

Tue, 23 May 2023 17:06:00 UTC (2,490 KB)

[v2]

Wed, 11 Oct 2023 05:27:50 UTC (2,491 KB)

Full-text links:

Access Paper:

View a PDF of the paper titled FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation, by Sewon Min and 8 other authors

View PDF

TeX Source

view license

Current browse context:

cs.CL

<Â prev

|

nextÂ >

new

|

recent

|

2023-05

Change to browse by:

cs

cs.AI

cs.LG

References & Citations

NASA ADS

Google Scholar

Semantic Scholar

1 blog link

(

what is this?

)

export BibTeX citation

Loading...

BibTeX formatted citation

Ã—

loading...

Data provided by:

Bookmark

Bibliographic Tools

Bibliographic and Citation Tools

Bibliographic Explorer Toggle

Bibliographic Explorer

(

What is the Explorer?

)

Connected Papers Toggle

Connected Papers

(

What is Connected Papers?

)

Litmaps Toggle

Litmaps

(

What is Litmaps?

)

scite.ai Toggle

scite Smart Citations

(

What are Smart Citations?

)

Code, Data, Media

Code, Data and Media Associated with this Article

alphaXiv Toggle

alphaXiv

(

What is alphaXiv?

)

Links to Code Toggle

CatalyzeX Code Finder for Papers

(

What is CatalyzeX?

)

DagsHub Toggle

DagsHub

(

What is DagsHub?

)

GotitPub Toggle

Gotit.pub

(

What is GotitPub?

)

Huggingface Toggle

Hugging Face

(

What is Huggingface?

)

Links to Code Toggle

Papers with Code

(

What is Papers with Code?

)

ScienceCast Toggle

ScienceCast

(

What is ScienceCast?

)

Demos

Demos

Replicate Toggle

Replicate

(

What is Replicate?

)

Spaces Toggle

Hugging Face Spaces

(

What is Spaces?

)

Spaces Toggle

TXYZ.AI

(

What is TXYZ.AI?

)

Related Papers

Recommenders and Search Tools

Link to Influence Flower

Influence Flower

(

What are Influence Flowers?

)

Core recommender toggle

CORE Recommender

(

What is CORE?

)

Author

Venue

Institution

Topic

About arXivLabs

arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community?

Learn more about arXivLabs

.

Which authors of this paper are endorsers?

|

Disable MathJax

(

What is MathJax?

)

---

## âœ“ Cost of a data breach 2025 | IBM

**URL:** https://www.ibm.com/reports/data-breach

Cost of a Data Breach Report 2025

Download report

Watch the webinar

The AI oversight gap

New global research from IBM and Ponemon Institute reveals how AI is greatly outpacing security and governance in favor of do-it-now adoption. The findings show that ungoverned AI systems are more likely to be breached and more costly when they are.

4.4M

The global average cost of a data breach, in USD, a 9% decrease over last yearâ€”driven by faster identification and containment.

97%

Share of organizations that reported an AI-related security incident and lacked proper AI access controls.

63%

Share of organizations that lacked AI governance policies to manage AI or prevent the proliferation of shadow AI.

1.9M

Cost savings, in USD, from extensive use of AI in security, compared to organizations that didnâ€™t use these solutions.

Key takeaways

Join IBM cybersecurity expert Jeff Crume as he unpacks this yearâ€™s key findings, strategic takeaways and recommendations for how you can limit risk and safeguard your AI, data, people and infrastructure.

Watch video (15:26)

Take action

Identity security

Identity security

Data security

Data security

AI oversight

AI oversight

Security automation

Security automation

Improve resilience

Improve resilience

Fortify identitiesâ€”humans and machines

AI and automation can fortify identity security without overburdening understaffed teams. Implementing strong operational controls for non-human identities (NHIs) and adopting modern, phishing-resistant authentication methods, such as passkeys, can significantly reduce the risk of credential abuse.

Explore IBMÂ® Verify

Explore passwordless authentication

Elevate AI data security practices

Implement strong data security fundamentals: data discovery, classification, access control, encryption and key management. Leverage AI and data security to protect data integrity and avoid compromise. These measures are essential as AI becomes both a threat vector and a security tool.

Explore IBM GuardiumÂ®

Secure AI models and agents

Connect security and governance for AI

Investing in integrated security and governance solutions allows organizations to gain visibility into all AI deployments (including shadow AI), mitigate vulnerabilities, protect prompts and data and use observability tools to improve compliance and detect anomalies.

Try watsonx.governanceÂ®

Explore IBM Guardium AI security

Use AI security tools and automation

As attackers use AI for more adaptive attacks, security teams must also embrace AI technologies. AI-powered security tools and services can reduce alert volume, identify at-risk data, spot security gaps, detect breaches early and enable faster, more precise responses.

Try detection and response

Use autonomous security

Prepare for cyber threats

Building resilience means quick detection and containment of security issues. Effective crisis response means regularly testing incident response (IR) plans and backups, defining clear roles in the event of a breach and conducting crisis simulations.

Join IBMâ€™s cyber range

Build your cybersecurity skills

Fortify identitiesâ€”humans and machines

AI and automation can fortify identity security without overburdening understaffed teams. Implementing strong operational controls for non-human identities (NHIs) and adopting modern, phishing-resistant authentication methods, such as passkeys, can significantly reduce the risk of credential abuse.

Explore IBMÂ® Verify

Explore passwordless authentication

Elevate AI data security practices

Implement strong data security fundamentals: data discovery, classification, access control, encryption and key management. Leverage AI and data security to protect data integrity and avoid compromise. These measures are essential as AI becomes both a threat vector and a security tool.

Explore IBM GuardiumÂ®

Secure AI models and agents

Connect security and governance for AI

Investing in integrated security and governance solutions allows organizations to gain visibility into all AI deployments (including shadow AI), mitigate vulnerabilities, protect prompts and data and use observability tools to improve compliance and detect anomalies.

Try watsonx.governanceÂ®

Explore IBM Guardium AI security

Use AI security tools and automation

As attackers use AI for more adaptive attacks, security teams must also embrace AI technologies. AI-powered security tools and services can reduce alert volume, identify at-risk data, spot security gaps, detect breaches early and enable faster, more precise responses.

Try detection and response

Use autonomous security

Prepare for cyber threats

Building resilience means quick detection and containment of security issues. Effective crisis response means regularly testing incident response (IR) plans and backups, defining clear roles in the event of a breach and conducting crisis simulations.

Join IBMâ€™s cyber range

Build your cybersecurity skills

Mixture of Experts

Suja Viswesan, Vice President,

[Content truncated...]

---

## âœ“ [2109.07958] TruthfulQA: Measuring How Models Mimic Human Falsehoods

**URL:** https://arxiv.org/abs/2109.07958

Computer Science > Computation and Language

arXiv:2109.07958

(cs)

[Submitted on 8 Sep 2021 (

v1

), last revised 8 May 2022 (this version, v2)]

Title:

TruthfulQA: Measuring How Models Mimic Human Falsehoods

Authors:

Stephanie Lin

,

Jacob Hilton

,

Owain Evans

View a PDF of the paper titled TruthfulQA: Measuring How Models Mimic Human Falsehoods, by Stephanie Lin and 2 other authors

View PDF

Abstract:

We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.

Comments:

ACL 2022 (main conference); the TruthfulQA benchmark and evaluation code is available at

this https URL

Subjects:

Computation and Language (cs.CL)

; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Machine Learning (cs.LG)

Cite as:

arXiv:2109.07958

[cs.CL]

(or

arXiv:2109.07958v2

[cs.CL]

for this version)

https://doi.org/10.48550/arXiv.2109.07958

Focus to learn more

arXiv-issued DOI via DataCite

Submission history

From: Stephanie Lin [

view email

]

[v1]

Wed, 8 Sep 2021 17:15:27 UTC (793 KB)

[v2]

Sun, 8 May 2022 02:43:02 UTC (7,171 KB)

Full-text links:

Access Paper:

View a PDF of the paper titled TruthfulQA: Measuring How Models Mimic Human Falsehoods, by Stephanie Lin and 2 other authors

View PDF

TeX Source

view license

Current browse context:

cs.CL

<Â prev

|

nextÂ >

new

|

recent

|

2021-09

Change to browse by:

cs

cs.AI

cs.CY

cs.LG

References & Citations

NASA ADS

Google Scholar

Semantic Scholar

2 blog links

(

what is this?

)

DBLP

- CS Bibliography

listing

|

bibtex

Owain Evans

export BibTeX citation

Loading...

BibTeX formatted citation

Ã—

loading...

Data provided by:

Bookmark

Bibliographic Tools

Bibliographic and Citation Tools

Bibliographic Explorer Toggle

Bibliographic Explorer

(

What is the Explorer?

)

Connected Papers Toggle

Connected Papers

(

What is Connected Papers?

)

Litmaps Toggle

Litmaps

(

What is Litmaps?

)

scite.ai Toggle

scite Smart Citations

(

What are Smart Citations?

)

Code, Data, Media

Code, Data and Media Associated with this Article

alphaXiv Toggle

alphaXiv

(

What is alphaXiv?

)

Links to Code Toggle

CatalyzeX Code Finder for Papers

(

What is CatalyzeX?

)

DagsHub Toggle

DagsHub

(

What is DagsHub?

)

GotitPub Toggle

Gotit.pub

(

What is GotitPub?

)

Huggingface Toggle

Hugging Face

(

What is Huggingface?

)

Links to Code Toggle

Papers with Code

(

What is Papers with Code?

)

ScienceCast Toggle

ScienceCast

(

What is ScienceCast?

)

Demos

Demos

Replicate Toggle

Replicate

(

What is Replicate?

)

Spaces Toggle

Hugging Face Spaces

(

What is Spaces?

)

Spaces Toggle

TXYZ.AI

(

What is TXYZ.AI?

)

Related Papers

Recommenders and Search Tools

Link to Influence Flower

Influence Flower

(

What are Influence Flowers?

)

Core recommender toggle

CORE Recommender

(

What is CORE?

)

Author

Venue

Institution

Topic

About arXivLabs

arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community?

Learn more about arXivLabs

.

Which authors of this paper are endorsers?

|

Disable MathJax

(

What is MathJax?

)

---

## âœ“ Google Researchers Find the Best AI Model Is 69% Right - Business Insider

**URL:** https://www.businessinsider.com/google-researchers-find-best-ai-model-69-right-2025-12?utm_source=chatgpt.com

Business Insider tells the innovative stories you want to know

---

## âœ“ Reddit - The heart of the internet

**URL:** https://www.reddit.com/r/MachineLearning/comments/1qcgh6d/d_classification_of_low_resource_language_using/

Go to MachineLearning

r/MachineLearning

â€¢

Sikandarch

[D]  Classification of low resource language using Deep learning

I have been trying to solve classification problem on a low resource language. I am doing comparative analysis, LinearSVC and Logistic regression performed the best and the only models with 80+ accuracy and no overfitting. I have to classify it using deep learning model as well. I applied BERT on the dataset, model is 'bert-base-multilingual-cased', and I am fine tuning it, but issue is overfitting.

Training logs:

Epoch 6/10 | Train Loss: 0.4135 | Train Acc: 0.8772 | Val Loss: 0.9208 | Val Acc: 0.7408

Epoch 7/10 | Train Loss: 0.2984 | Train Acc: 0.9129 | Val Loss: 0.8313 | Val Acc: 0.7530

Epoch 8/10 | Train Loss: 0.2207 | Train Acc: 0.9388 | Val Loss: 0.8720 | Val Acc: 0.7505

this was with default dropout of the model, when I change dropout to 0.3, or even 0.2, model still overfits but not this much, but with dropout I don't go near 60% accuracy, long training introduces overfitting, early stopping isn't working as val loss continuous to decrease. On 10 epoch, I trained patience of 2 and 3. It doesn't stops. To prevent this I am not doing warmup step, my optimizer is below:

optimizer = AdamW([

{'params': model.bert.parameters(), 'lr': 2e-5},

{'params': model.classifier.parameters(), 'lr': 3e-5}

], weight_decay=0.01)

About my dataset,

I have 9000 training samples and 11 classes to train, data is imbalanced but not drastically, to cater this I have added class weights to loss function.

17 words per training sample on average. I set the max_length to 120 for tokens ids and attention masks.

How can I improve my training, I am trying to achieve atleast 75% accuracy without overfitting, for my comparative analysis. What I am doing wrong? Please guide me.

Data Augmentation didn't work too. I did easy data augmentation. Mixup Augmentation also didn't work.

If you need more information about my training to answer questions, ask in the comment, thanks.

Read more

Share

---

## âœ“ The Uncomfortable Truth About AI Agents: 90% Claim Victory While 10% Achieve Adoption | Tech Upkeep | Tech Upkeep - Newsletter for Engineers

**URL:** https://www.techupkeep.dev/blog/state-of-agentic-ai-2025?utm_source=chatgpt.com

Back to Blog

The agentic AI market has reached its inflection point, but not the one you think. While MMC's latest survey shows 90% of agentic startups claiming over 70% accuracy, only 10% of enterprises report "significant adoption" with actual employee integration. The gap reveals a strategic miscalculation that will eliminate 40% of agent initiatives by 2027.

The evidence is unambiguous: 42% of organizations deployed "at least some agents" in Q3 2025, up from 11% two quarters prior. CFOs allocated 25% of total AI budgets to agents. Yet 68% of employees interact with agents in fewer than half their workflows. The uncomfortable truth is that accuracy was never the bottleneck.

The Gap: When 90% Accuracy Meets 10% Adoption

MMC surveyed 30+ European agentic AI founders and 40+ enterprise practitioners. The findings expose a fundamental disconnect between vendor promises and enterprise reality:

Healthcare startups report 90% accuracy rates. Financial services averages 80%. Yet healthcare founders themselves admit: "This accuracy level is not sufficient to remove human oversight." The irony is palpable - the sectors with highest accuracy maintain lowest autonomy levels, typically 40%.

Deployment Patterns

â€¢

Healthcare:

90% accuracy, 40% autonomy

â€¢

Financial services:

80% accuracy, 70% autonomy

â€¢

General enterprise:

70% accuracy, 70% autonomy

The pattern inverts conventional wisdom. Higher accuracy correlates with lower autonomy in production. Healthcare founders openly admit they "downplay AI terminology" and focus on "operational benefits" instead - a deliberate strategic positioning. One founder confessed: "If you use the words 'agent' or 'AI' it backlashes more than it benefits."

Why It's Happening: The Triple Infrastructure Trap

1. The Integration Nightmare (60% cite as primary blocker)

The median agentic startup requires integration with 8+ data sources. 52% built their infrastructure entirely in-house because existing tools couldn't handle the complexity. The most-used framework? LangChain - itself only 18 months old.

One founder's confession crystallizes the problem: "Supporting multiple unique instances...the last mile UI is probably the biggest headache." The challenge extends beyond API connections to retrofitting agent workflows into ServiceNow, Slack, and legacy systems simultaneously while maintaining coherent user experiences.

2. The Reasoning Token Bomb

OpenAI's o1 and similar reasoning models changed everything. These models produce 5x longer outputs annually and consume 8x more tokens than standard models. Internal reasoning alone burns ~5,000 tokens to produce a 100-token response.

The math is brutal:

A startup achieving 80% accuracy with GPT-4 at $10/million tokens suddenly needs reasoning models at $60/million tokens consuming 8x volume. That's a 48x cost increase for 10% accuracy gain. One founder admitted: "Model consistency challenges limit infrastructure margin through required multi-pass reasoning models for 2025 reliability standards."

3. The Human Resistance Factor (50% report as blocker)

The survey exposes what vendors won't discuss: employee resistance centers on trust allocation, not job replacement fears. 45% of deployments show "slight adoption" where employees are "beginning integration." The gap between "beginning" and "significant" remains fundamentally human, not technological.

MMC's data reveals the paradox: Companies emphasize "co-pilot positioning" even when full autonomy is technically possible. They discovered employees either overrely or underrely on outputs - never achieving optimal collaboration. The sweet spot remains elusive.

Strategic Implications: The 2027 Reckoning

Gartner predicts over 40% of agent-based AI initiatives will be abandoned by end of 2027. The MMC data suggests they're optimistic.

The Pricing Reality Check

Only 3% of startups attempt outcome-based pricing - the supposed "Holy Grail." The other 97% know what vendors won't admit: outcome attribution in complex workflows is impossible. The market settled on hybrid models (23%) and per-task pricing (23%) not from lack of ambition but from operational reality.

62% of agentic startups now tap Line of Business budgets rather than innovation funds. This shift from experimental to operational spend creates a new dynamic: ROI requirements are immediate, not aspirational.

The Workflow Integration Wall

MMC found successful deployments share specific characteristics:

âœ“

Target "low-risk yet medium-impact" use cases

âœ“

Automate tasks employees actively dislike

âœ“

Ensure easily verifiable outputs

âœ“

Demonstrate ROI within one quarter

The pattern is clear: Winners aren't building autonomous systems. They're building narrow, high-frequency task executors with human oversight. One founder summarized: "If you give people a browser saying it can do anything on the web, they'll expect Amazon product scraping at scale."

Timeline: The Next 18 Months

Q4 2025 - Q1 2026: Consolidation 

[Content truncated...]

---

## âœ“ Sign Up | LinkedIn

**URL:** https://www.linkedin.com/posts/paveen-reddy-katakam-05a57b233_googles-facts-benchmark-broke-ai-confidence-activity-7414233812429099008-7NIB

Not you?

Remove photo

Email or phone number

Password

Show

Keep me logged in

First name

Last name

Agree & Join LinkedIn

By clicking Continue, you agree to LinkedInâ€™s

User Agreement

,

Privacy Policy

, and

Cookie Policy

.

By clicking Agree & Join or Continue, you agree to the LinkedIn

User Agreement

,

Privacy Policy

, and

Cookie Policy

.

Agree & Join

or

Already on LinkedIn?

Sign in

Looking to create a page for a business?

Get help

---

## âœ“ AI hallucinates more frequently the more advanced it gets. Is there any way of stopping it? | Live Science

**URL:** https://www.livescience.com/technology/artificial-intelligence/ai-hallucinates-more-frequently-as-it-gets-more-advanced-is-there-any-way-to-stop-it-from-happening-and-should-we-even-try?utm_source=chatgpt.com

(Image credit: agsandrew/ Shutterstock)

Share

Share by:

Copy link

Facebook

X

Whatsapp

Reddit

Pinterest

Flipboard

Email

Share this article

13

Join the conversation

Follow us

Add us as a preferred source on Google

Newsletter

Subscribe to our newsletter

The more advanced

artificial intelligence

(AI) gets, the more it "hallucinates" and provides incorrect and inaccurate information.

Research

conducted by OpenAI found that its latest and most powerful reasoning models, o3 and o4-mini, hallucinated 33% and 48% of the time, respectively, when tested by OpenAI's PersonQA benchmark. That's more than double the rate of the older o1 model. While o3 delivers more accurate information than its predecessor, it appears to come at the cost of more inaccurate hallucinations.

This raises a concern over the accuracy and reliability of large language models (LLMs) such as AI chatbots, said

Eleanor Watson

, an Institute of Electrical and Electronics Engineers (IEEE) member and AI ethics engineer at Singularity University.

You may like

Switching off AI's ability to lie makes it more likely to claim it's conscious, eerie study finds

'It wonâ€™t be so much a ghost town as a zombie apocalypse': How AI might forever change how we use the internet

Even AI has trouble figuring out if text was written by AI â€” here's why

"When a system outputs fabricated information â€” such as invented facts, citations or events â€” with the same fluency and coherence it uses for accurate content, it risks misleading users in subtle and consequential ways," Watson told Live Science.

Related:

Cutting-edge AI models from OpenAI and DeepSeek undergo 'complete collapse' when problems get too difficult, study reveals

The issue of hallucination highlights the need to carefully assess and supervise the information AI systems produce when using LLMs and reasoning models, experts say.

Do AIs dream of electric sheep?

The crux of a reasoning model is that it can handle complex tasks by essentially breaking them down into individual components and coming up with solutions to tackle them. Rather than seeking to kick out answers based on statistical probability, reasoning models come up with strategies to solve a problem, much like how humans think.

Sign up for the Live Science daily newsletter now

Get the worldâ€™s most fascinating discoveries delivered straight to your inbox.

Contact me with news and offers from other Future brands

Receive email from us on behalf of our trusted partners or sponsors

In order to develop creative, and potentially novel, solutions to problems, AI needs to hallucinate â€”otherwise it's limited by rigid data its LLM ingests.

"It's important to note that hallucination is a feature, not a bug, of AI,"

Sohrob Kazerounian

, an AI researcher at Vectra AI, told Live Science. "To paraphrase a colleague of mine, 'Everything an LLM outputs is a hallucination. It's just that some of those hallucinations are true.' If an AI only generated verbatim outputs that it had seen during training, all of AI would reduce to a massive search problem."

"You would only be able to generate computer code that had been written before, find proteins and molecules whose properties had already been studied and described, and answer homework questions that had already previously been asked before. You would not, however, be able to ask the LLM to write the lyrics for a concept album focused on the AI singularity, blending the lyrical stylings of Snoop Dogg and Bob Dylan."

You may like

Switching off AI's ability to lie makes it more likely to claim it's conscious, eerie study finds

'It wonâ€™t be so much a ghost town as a zombie apocalypse': How AI might forever change how we use the internet

Even AI has trouble figuring out if text was written by AI â€” here's why

In effect, LLMs and the AI systems they power need to hallucinate in order to create, rather than simply serve up existing information. It is similar, conceptually, to the way that humans dream or imagine scenarios when conjuring new ideas.

Thinking too much outside the box

However,

AI hallucinations

present a problem when it comes to delivering accurate and correct information, especially if users take the information at face value without any checks or oversight.

"This is especially problematic in domains where decisions depend on factual precision, like medicine, law or finance," Watson said. "While more advanced models may reduce the frequency of obvious factual mistakes, the issue persists in more subtle forms. Over time, confabulation erodes the perception of AI systems as trustworthy instruments and can produce material harms when unverified content is acted upon."

And this problem looks to be exacerbated as AI advances. "As model capabilities improve, errors often become less overt but more difficult to detect," Watson noted. "Fabricated content is increasingly embedded within plausible narratives and coherent reasoning chains. This introduces a particular ri

[Content truncated...]

---

## âœ“ OpenAI CEO Sam Altman says Reddit, Twitter feels Ã¢Â€Â˜fakeÃ¢Â€Â™; raises concerns over AI-driven posts - The Times of India

**URL:** https://timesofindia.indiatimes.com/technology/social/openai-ceo-sam-altman-says-reddit-twitter-feels-fake-raises-concerns-over-ai-driven-posts/articleshow/123782801.cms?utm_source=chatgpt.com

Edition

IN

IN

US

English

English

Ã Â¤Â¹Ã Â¤Â¿Ã Â¤Â¨Ã Â¥ÂÃ Â¤Â¦Ã Â¥Â€

Ã Â¤Â®Ã Â¤Â°Ã Â¤Â¾Ã Â¤Â Ã Â¥Â€

Ã Â²Â•Ã Â²Â¨Ã Â³ÂÃ Â²Â¨Ã Â²Â¡

Ã Â®Â¤Ã Â®Â®Ã Â®Â¿Ã Â®Â´Ã Â¯Â

Ã Â¦Â¬Ã Â¦Â¾Ã Â¦Â‚Ã Â¦Â²Ã Â¦Â¾

Ã Â´Â®Ã Â´Â²Ã Â´Â¯Ã Â´Â¾Ã Â´Â³Ã Â´Â‚

Ã Â°Â¤Ã Â±Â†Ã Â°Â²Ã Â±ÂÃ Â°Â—Ã Â±Â

Ã ÂªÂ—Ã Â«ÂÃ ÂªÂœÃ ÂªÂ°Ã ÂªÂ¾Ã ÂªÂ¤Ã Â«Â€

Weather

Sign In

TOI

Today's ePaper

News

Technology News

Social News

OpenAI CEO Sam Altman says Reddit, Twitter feels Ã¢Â€Â˜fakeÃ¢Â€Â™; raises concerns over AI-driven posts

Trending

NYT Connections Hints

Wordle Today Clues

NYT Strands

Mark Zuckerberg

Jensen Huang

Apple iPhone 17

Sundar Pichai

Elon Musk

Kairan Quazi

jeff Bezos

NYT Connections Hints

Wordle Today Clues

NYT Strands

Mark Zuckerberg

Jensen Huang

Apple iPhone 17

Sundar Pichai

Elon Musk

Kairan Quazi

jeff Bezos

NYT Connections Hints

Wordle Today Clues

NYT Strands

Mark Zuckerberg

Jensen Huang

Apple iPhone 17

Sundar Pichai

Elon Musk

Kairan Quazi

jeff Bezos

OpenAI CEO Sam Altman says Reddit, Twitter feels Ã¢Â€Â˜fakeÃ¢Â€Â™; raises concerns over AI-driven posts

TOI Tech Desk

/ TIMESOFINDIA.COM /

Updated: Sep 10, 2025, 11:22 IST

Share

AA

+

Text Size

Small

Medium

Large

OpenAI CEO Sam Altman expresses concern over the rise of AI bots. He finds it difficult to distinguish between genuine and AI-generated content on social media. Altman attributes this to factors like LLM-speak adoption and social media engagement tactics. He notes that online conversations feel less authentic now.

OpenAI

CEO

Sam Altman

recently said that AI bots have made it nearly impossible to know whether posts on social media are written by real people. Sharing a post on X (formerly

Twitter

), Altman said Ã¢Â€ÂœI have had the strangest experience reading this: I assume itÃ¢Â€Â™s all fake/bots, even though in this case I know codex growth is really strong and the trend here is realÃ¢Â€Â.

Sam Altman was responding to an activity on the r/Claudecode subreddit, where many users claimed they had switched to OpenAIÃ¢Â€Â™s Codex tool.

In the post, the OpenAI CEO admitted that he could not tell whether the praise was genuine or generated.

Elon MuskÃ¢Â€Â™s Own AI Turns on Him, Publicly Sides With Sam Altman | WATCH

Ã¢Â€ÂœI have had the strangest experience reading this: i assume its all fake/bots, even though in this case i know codex growth is really strong and the trend here is realÃ¢Â€Â.

Why Sam Altman is concerned

In the post, Sam Altman explained that several factors contribute to the problem: people copying Ã¢Â€ÂœLLM-speak,Ã¢Â€Â online groups behaving in highly similar ways, social media platforms rewarding engagement, and companies using Ã¢Â€ÂœastroturfingÃ¢Â€Â tactics.

Ã¢Â€Âœi think there are a bunch of things going on: real people have picked up quirks of LLM-speak, the Extremely Online crowd drifts together in very correlated ways, the hype cycle has a very "it's so over/we're so back" extremism, optimization pressure from social platforms on juicing engagement and the related way that creator monetization works,Ã¢Â€Â he stated, adding Ã¢Â€Âœother companies have astroturfed us so IÃ¢Â€Â™m extra sensitive to it, and a bunch more (including probably some bots)Ã¢Â€Â.

He also noted that the rise of AI has made online conversations feel less authentic. Ã¢Â€ÂœThe net effect is somehow AI twitter/AI Reddit feels very fake in a way it really didnÃ¢Â€Â™t a year or two ago,Ã¢Â€Â he said.

Last week, Sam Altman shared an X post where he said that he never took the dead internet theory seriously, but now it seems Ã¢Â€Âœlike there are really a lot of LLM-run twitter accounts nowÃ¢Â€Â

Inbase Torque Review: Budget Speaker That Will Surprise You

About the Author

TOI Tech Desk

The TOI Tech Desk is a dedicated team of journalists committed to delivering the latest and most relevant news from the world of technology to readers of The Times of India. TOI Tech DeskÃ¢Â€Â™s news coverage spans a wide spectrum across gadget launches, gadget reviews, trends, in-depth analysis, exclusive reports and breaking stories that impact technology and the digital universe. Be it how-tos or the latest happenings in AI, cybersecurity, personal gadgets, platforms like WhatsApp, Instagram, Facebook and more; TOI Tech Desk brings the news with accuracy and authenticity.

Read More

End of Article

Follow Us On Social Media

Visual Stories

Previous

Why these 5 skincare ingredients should only work the night shift

Lifestyle

How to train a Labrador Retriever puppy: 10 easy steps every owner should follow

Lifestyle

Celebrities who turned vegetarian for roles in surprising ways

Entertainment

Priyanka Chahar ChaudharyÃ¢Â€Â™s inspired stunning looks

tv

Aishwarya Rai Bachchan's steal-worthy saree collection

Entertainment

9 Films where the villain stole the show from the hero

Entertainment

Kajal Aggarwal's saree looks to take fashion inspiration

Entertainment

'Blackmail' actress Teju Ashwini's gorgeous clicks

Entertainment

Highest-selling car manufacturers in August 2025: Maruti, Tata, Hyundai and more

auto

Krithi Shetty donning all-white outfit in style

Entertainment

Next

1

2

3

Photostories

From earning Rs 700 per performance 

[Content truncated...]

---

## âœ“ The state of AI in early 2024 | McKinsey

**URL:** https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-2024

The state of AI in early 2024: Gen AI adoption spikes and starts to generate value

May 30, 2024

| Survey

As generative AI adoption accelerates, survey respondents report measurable benefits and increased mitigation of the risk of inaccuracy. A small group of high performers lead the way.

YouÃ¢Â€Â™ve reached  reached a page with older survey data. Please see our latest survey results

here

.

(23 pages)

If 2023

was the year the world discovered

generative AI (gen AI)

, 2024 is the year organizations truly began usingÃ¢Â€Â”and deriving business value fromÃ¢Â€Â”this new technology. In the latest

McKinsey Global Survey

Ã‚Â on AI, 65 percent of respondents report that their organizations are regularly using gen AI, nearly double the percentage from our previous survey just ten months ago. RespondentsÃ¢Â€Â™ expectations for gen AIÃ¢Â€Â™s impact remain as high

as they were last year

, with three-quarters predicting that gen AI will lead to significant or disruptive change in their industries in the years ahead.

About the authors

This article is a collaborative effort by

Alex Singla

,

Alexander Sukharevsky

,

Lareina Yee

, and Michael Chui, with

Bryce Hall

, representing views from QuantumBlack, AI by McKinsey, and McKinsey Digital.

Organizations are already seeing material benefits from gen AI use, reporting both cost decreases and revenue jumps in the business units deploying the technology. The survey also provides insights into the kinds of risks presented by gen AIÃ¢Â€Â”most notably, inaccuracyÃ¢Â€Â”as well as the emerging practices of top performers to mitigate those challenges and capture value.

AI adoption surges

Interest in generative AI has also brightened the spotlight on a broader set of AI capabilities. For the past six years, AI adoption by respondentsÃ¢Â€Â™ organizations has hovered at about 50 percent. This year, the survey finds that adoption has jumped to 72 percent (Exhibit 1). And the interest is truly global in scope. Our 2023 survey found that AI adoption did not reach 66 percent in

any

region; however, this year more than two-thirds of respondents in nearly

every

region say their organizations are using AI.

1

Organizations based in Central and South America are the exception, with 58 percent of respondents working for organizations based in Central and South America reporting AI adoption.

Looking by industry, the biggest increase in adoption can be found in professional services.

2

Includes respondents working for organizations focused on human resources, legal services, management consulting, market research, R&D, tax preparation, and training.

Also, responses suggest that companies are now using AI in more parts of the business. Half of respondents say their organizations have adopted AI in two or more business functions, up from less than a third of respondents in 2023 (Exhibit 2).

Gen AI adoption is most common in the functions where it can create the most value

Most respondents now report that their organizationsÃ¢Â€Â”and they as individualsÃ¢Â€Â”are using gen AI. Sixty-five percent of respondents say their organizations are regularly using gen AI in at least one business function, up from one-third last year. The average organization using gen AI is doing so in two functions, most often in marketing and sales and in product and service developmentÃ¢Â€Â”two functions in which

previous research

Ã‚Â determined that gen AI adoption could generate the most value

3

Ã¢Â€Âœ

The economic potential of generative AI: The next productivity frontier

,Ã¢Â€Â McKinsey, June 14, 2023.

Ã¢Â€Â”as well as in IT (Exhibit 3). The biggest increase from 2023 is found in marketing and sales, where reported adoption has more than doubled. Yet across functions, only two use cases, both within marketing and sales, are reported by 15 percent or more of respondents.

Gen AI also is weaving its way into respondentsÃ¢Â€Â™ personal lives. Compared with 2023, respondents are much more likely to be using gen AI at work and even more likely to be using gen AI both at work and in their personal lives (Exhibit 4). The survey finds upticks in gen AI use across all regions, with the largest increases in AsiaÃ¢Â€Â“Pacific and Greater China. Respondents at the highest seniority levels, meanwhile, show larger jumps in the use of gen Al tools for work and outside of work compared with their midlevel-management peers. Looking at specific industries, respondents working in energy and materials and in professional services report the largest increase in gen AI use.

Investments in gen AI and analytical AI are beginning to create value

The latest survey also shows how different industries are budgeting for gen AI. Responses suggest that, in many industries, organizations are about equally as likely to be investing more than 5 percent of their digital budgets in gen AI as they are in nongenerative, analytical-AI solutions (Exhibit 5). Yet in most industries, larger shares of respondents report that their organizations spend more than 20 perc

[Content truncated...]

---

## âœ“ https://hai.stanford.edu/assets/files/hai_ai-index-report-2025_chapter3_final.pdf?utm_source=chatgpt.com

**URL:** https://hai.stanford.edu/assets/files/hai_ai-index-report-2025_chapter3_final.pdf?utm_source=chatgpt.com

%PDF-1.7%ï¿½ï¿½ï¿½ï¿½

13003 0 obj<

>endobj     xref

13003 26

0000000016 00000 n

0000003214 00000 n

0000003369 00000 n

0000003409 00000 n

0000004483 00000 n

0000004524 00000 n

0000004640 00000 n

0000005724 00000 n

0000006151 00000 n

0000006610 00000 n

0000007007 00000 n

0000007094 00000 n

0000007195 00000 n

0000008010 00000 n

0000008686 00000 n

0000009402 00000 n

0000009720 00000 n

0000010031 00000 n

0000010378 00000 n

0000012960 00000 n

0000013753 00000 n

0000016404 00000 n

0000021769 00000 n

0000331075 00000 n

0001150476 00000 n

0000000816 00000 n

trailer<

]/Prev 9442494>>startxref0%%EOF     13028 0 obj<

>stream

hï¿½ï¿½Wypï¿½+i/I6ï¿½ï¿½ï¿½ï¿½ï¿½wï¿½Zï¿½ï¿½ï¿½Eï¿½`CH [Aï¿½BHï¿½l\bZPï¿½ï¿½:ï¿½Ã¦@!ï¿½8`ï¿½HL!ï¿½Ñ‹&ï¿½ï¿½ï¿½6

ï¿½ï¿½3ï¿½Nï¿½H&ï¿½ï¿½c:ï¿½ï¿½ï¿½hï¿½ig:}ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Vï¿½ï¿½ï¿½ï¿½]ï¿½ï¿½ï¿½ï¿½ ï¿½ ï¿½ï¿½Pà­€J(>ï¿½ï¿½ .ï¿½Leï¿½É†czï¿½3;ï¿½.ï¿½ï¿½ ï¿½(p&ï¿½`.ï¿½ï¿½ï¿½ï¿½ï¿½!zï¿½Lï¿½r$*`!$<ï¿½ï¿½Kgï¿½ï¿½jï¿½I8ï¿½ï¿½ï¿½ï¿½S>ï¿½ï¿½uf 2Tvï¿½ï¿½cï¿½ï¿½$ï¿½ï¿½ï¿½O)ï¿½ï¿½ï¿½]2ï¿½dï¿½Óµï¿½iï¿½ï¿½ï¿½ï¿½ï¿½ï¿½

á¬©sß“h|ï¿½ï¿½N:vï¿½ï¿½gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ì©3ï¿½ï¿½ï¿½tï¿½ï¿½Ýšï¿½ï¿½uï¿½ï¿½sï¿½ï¿½ï¿½ï¿½ï¿½ï¿½f{ï¿½ï¿½Nï¿½rï¿½ï¿½ï¿½ï¿½Bï¿½ï¿½ï¿½ï¿½ï¿½Jï¿½ï¿½ï¿½6ï¿½ï¿½=ï¿½OTã©š/ï¿½pï¿½ï¿½y[S;sï¿½Wï¿½xï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Aï¿½6jiï¿½ï¿½ï¿½ï¿½w_Y1vï¿½ï¿½ï¿½ï¿½wï¿½ï¿½mï¿½ï¿½wÇ®u×¶nï¿½sï¿½ï¿½ï¿½ï¿½ï¿½ï¿½jwï¿½ï¿½ï¿½gï¿½=vï¿½U9ï¿½N^ï¿½xÝ’"34Ë…ï¿½ï¿½ï¿½_ï¿½:ï¿½Þ­)+ï¿½ï¿½Wï¿½ï¿½>!ï¿½ï¿½ï¿½Ã©ï¿½2ï¿½ï¿½jyï¿½ï¿½ï¿½ï¿½ "ï¿½ß‡:ï¿½ï¿½)ï¿½ï¿½jNï¿½w{ï¿½ï¿½$Fï¿½\ï¿½5hï¿½Î»ï¿½ï¿½ï¿½ï¿½~!P+ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½u#Gï¿½ï¿½DÃ†ï¿½$lï¿½ï¿½Iï¿½ IS%Y"$ï¿½ï¿½D;*ï¿½ï¿½Tï¿½(mï¿½.8ï¿½ï¿½ï¿½ï¿½{=cFsAï¿½ï¿½ï¿½Ñ·ï¿½ï¿½Cï¿½Úˆ&ï¿½qï¿½Mï¿½5(ï¿½ï¿½Ø»ï¿½ï¿½ï¿½EXÔˆUï¿½ï¿½Z$.bï¿½ï¿½ï¿½{ï¿½ï¿½"ï¿½0ï¿½qï¿½ï¿½Pï¿½:ï¿½ï¿½ï¿½]ï¿½ï¿½ï¿½ï¿½O0ï¿½ï¿½ï¿½ï¿½Xzï¿½ï¿½ï¿½ï¿½hï¿½#ï¿½ï¿½ï¿½K4Md9Zaï¿½ï¿½rï¿½ï¿½â‚¿Qï¿½!Sï¿½ï¿½9ï¿½7ï¿½ï¿½e5ï¿½ï¿½ï¿½#Zc@ï¿½rt$gï¿½ï¿½i_ï¿½ï¿½ï¿½ï¿½

Apï¿½5ï¿½lÄŽï¿½ï¿½Ù³à¢¬ï¿½ï¿½ï¿½ï¿½g6ï¿½O@ï¿½$Tï¿½ï¿½ï¿½ï¿½9_ï¿½FSï¿½qï¿½P	eï¿½ADKï¿½ï¿½v$ï¿½ï¿½ï¿½ï¿½)ï¿½aR*É±(ï¿½Pï¿½ï¿½ï¿½ï¿½O%&E$ï¿½aï¿½ï¿½0ï¿½

ï¿½ï¿½lï¿½ï¿½Tg(ï¿½!Dï¿½Mï¿½ï¿½ï¿½#ï¿½ï¿½=ï¿½ï¿½.ï¿½MTyï¿½ï¿½"ï¿½4ï¿½Uï¿½ï¿½-ï¿½;Ô•ï¿½ï¿½f[1ï¿½ï¿½ï¿½ï¿½ï¿½bÉ¨ï¿½Ý„[ï¿½1o4wï¿½ï¿½ï¿½ï¿½sï¿½v'ï¿½gï¿½ï¿½ï¿½Aï¿½K?ï¿½rï¿½2ï¿½0ï¿½Lï¿½;ï¿½oï¿½ï¿½=ï¿½ï¿½ï¿½lï¿½ï¿½ï¿½/ï¿½ï¿½ï¿½U4yï¿½oCï¿½DÚï¿½Xï¿½4ï¿½ï¿½gï¿½ï¿½/ï¿½*ï¿½ï¿½ï¿½ï¿½bQï¿½ï¿½ï¿½kï¿½ï¿½ï¿½ï¿½23Hï¿½ï¿½ï¿½ï¿½?-U[Ñ˜ï¿½lï¿½fï¿½:dï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Æ±Õ7~~ï¿½Ü›ØŠï¿½f+ï¿½]dï¿½Çï¿½-)ï¿½ï¿½;ï¿½ï¿½fï¿½0Xï¿½[ï¿½ï¿½ï¿½ï¿½1]6Ô¨tï¿½ï¿½ï¿½2Ü¯ï¿½cï¿½9qo!9ï¿½a ï¿½ß”ï¿½qLÖƒï¿½ï¿½ï¿½ï¿½{:ï¿½rï¿½;ï¿½>88DxCE×°ï¿½ï¿½ï¿½ï¿½$ï¿½ï¿½ï¿½ï¿½ï¿½S~ï¿½ï¿½~ï¿½7&yï¿½`~ï¿½ï¿½_rwï¿½ï¿½ï¿½.ï¿½ï¿½c6Hï¿½ï¿½8Ê’zG'2nï¿½;ï¿½ï¿½ï¿½cï¿½7ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½qï¿½ï¿½Ù€ï¿½ï¿½ï¿½ï¿½ï¿½dï¿½ï¿½Kï¿½Ë¡ï¿½ï¿½Â¾ï¿½}ï¿½?oï¿½ï¿½x9wï¿½Bï¿½ï¿½ï¿½+Zï¿½ï¿½Jï¿½ï¿½ï¿½ï¿½]ï¿½Ob:ï¿½ï¿½ï¿½ï¿½+ï¿½ï¿½ï¿½ï¿½wï¿½SÒï¿½%Qï¿½ï¿½Ä¤ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½Qï¿½ï¿½vXï¿½Aï¿½sï¿½mï¿½aï¿½}ï¿½ï¿½el#|ï¿½hZ4ï¿½BQï¿½jLï¿½)7$Sï¿½ï¿½ï¿½'ï¿½ï¿½mï¿½jï¿½ï¿½ï¿½wï¿½H=yR-ï¿½ï¿½IIï¿½lh%b>ï¿½ï¿½eQ+ï¿½Iï¿½ï¿½vï¿½[ ï¿½^Ô´ï¿½%ï¿½4ï¿½1]|

ï¿½ÓƒKï¿½Lï¿½ï¿½.vï¿½ï¿½Xï¿½ï¿½dï¿½ï¿½hï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Dï¿½Jï¿½hNï¿½oï¿½ï¿½Sï¿½ï¿½BoI~ï¿½ï¿½ï¿½ï¿½ï¿½}h>ï¿½ï¿½ÝŠï¿½ï¿½ï¿½>ß”Ò±AÂŠï¿½\Vï¿½×”ï¿½ï¿½KcU$ï¿½f]%cï¿½ï¿½ï¿½*ï¿½ï¿½Ý¡iZï¿½ï¿½ï¿½ï¿½ï¿½7ï¿½%9%ï¿½ï¿½Hco7ï¿½Lï¿½ï¿½ï¿½ï¿½.ï¿½ï¿½ï¿½ï¿½5ï¿½ï¿½KVï¿½<ï¿½ï¿½ï¿½ï¿½ï¿½mï¿½ï¿½+a{ï¿½Fï¿½ï¿½ï¿½ï¿½[ï¿½ï¿½Zï¿½ï¿½ï¿½ï¿½ ï¿½Lï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Zï¿½ï¿½ uï¿½upgï¿½ï¿½aï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½P]ï¿½ï¿½ï¿½gÕŽ?pï¿½ï¿½*8ï¿½-ï¿½fï¿½ï¿½ï¿½ï¿½p^ï¿½ï¿½#ï¿½rpeï¿½ï¿½3ï¿½Ë°ï¿½uï¿½ï¿½ï¿½Z`Ú½ï¿½ï¿½ï¿½6ï¿½ï¿½@ï¿½ï¿½Qï¿½*ï¿½/ï¿½ï¿½ï¿½Rï¿½sfA]Ý¿ï¿½ï¿½;ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½jï¿½R=Pï¿½G 8soï¿½ï¿½ï¿½!6Pï¿½9ï¿½rï¿½4ï¿½ï¿½:fï¿½$ï¿½ï¿½Yï¿½Cï¿½ï¿½ï¿½(ï¿½ï¿½ï¿½^ï¿½"ï¿½{ï¿½è£Žï¿½:"ï¿½@Ì€u$ï¿½ï¿½&ï¿½ï¿½ï¿½ï¿½Â½ï¿½ï¿½ ï¿½Q}ï¿½*ï¿½ï¿½ï¿½<*ï¿½f[ï¿½2ï¿½~ï¿½ï¿½ï¿½~yï¿½kï¿½ï¿½yï¿½7Cï¿½	Mï¿½ï¿½ï¿½1qï¿½ï¿½Õ´ï¿½Gï¿½ï¿½`Î¦ï¿½ï¿½ï¿½ï¿½Ê›ï¿½Sï¿½ ï¿½pUï¿½R@ï¿½RCï¿½2K-ï¿½É‚ï¿½ï¿½Oï¿½)pï¿½aï¿½_ï¿½ï¿½ï¿½ =ï¿½ï¿½ï¿½ï¿½^ï¿½Tï¿½ï¿½ï¿½ï¿½O~ï¿½ï¿½ï¿½ï¿½Kï¿½ï¿½5sï¿½ï¿½ï¿½ï¿½ï¿½k3ï¿½ï¿½uï¿½nï¿½ï¿½v}ï¿½ï¿½O;ï¿½=lï¿½ï¿½ï¿½ï¿½Ú‰ï¿½Ó¯]ï¿½ï¿½-ï¿½?Ô¬<ï¿½Kï¿½^ß¼ï¿½ï¿½ï¿½)oï¿½ï¿½WÍ´ï¿½/mï¿½ï¿½3ï¿½ï¿½ 0ï¿½F=ï¿½>Aï¿½ï¿½ï¿½ï¿½ï¿½aÇ…ï¿½ï¿½ï¿½tï¿½sï¿½ï¿½~ï¿½ï¿½ï¿½ï¿½

0 ï¿½ï¿½U endstreamendobj13004 0 obj<

>>>endobj13005 0 obj<

>endobj13006 0 obj<

>/ExtGState<

>/Font<

>/ProcSet[/PDF/Text/ImageC]/XObject<

>>>/Rotate 0/Tabs/W/Thumb 12926 0 R/TrimBox[0.0 0.0 612.0 792.0]/Type/Page/PieceInfo<

/LastModified

/NumberofPages 1/OriginalDocumentID

/PageTransformationMatrixList<

>/PageUIDList<

>/PageWidthList<

>>>>>>>endobj13007 0 obj[/ICCBased 13023 0 R]endobj13008 0 obj<

>endobj13009 0 obj<

>stream

Hï¿½lVMoï¿½6ï¿½ï¿½Wï¿½Q:,ï¿½ï¿½7{sÓ uOï¿½ï¿½@mQ8ï¿½:5*ï¿½ï¿½ï¿½ï¿½&ï¿½ï¿½oHï¿½Rï¿½baï¿½ï¿½ï¿½yï¿½Aï¿½Wï¿½~ï¿½4_H+f

Wï¿½ï¿½Û»ï¿½ï¿½a8ï¿½

ï¿½ï¿½ï¿½,iï¿½

ï¿½=Yï¿½ï¿½@ï¿½ï¿½$ï¿½b

t06+ï¿½lï¿½ï¿½GC.&ÅŽï¿½ï¿½ix~]`V9gï¿½!)V|hXï¿½ï¿½ï¿½eï¿½SÆšï¿½ï¿½ï¿½Xï¿½=ï¿½ Bï¿½ï¿½ï¿½ï¿½2ï¿½9[ï¿½"ï¿½ï¿½ï¿½Iï¿½wï¿½ï¿½$ï¿½ï¿½,ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½$ï¿½ï¿½ï¿½ï¿½ï¿½Qe=Eï¿½Tï¿½ï¿½ï¿½|BdNyï¿½1\ï¿½ï¿½W&DIï¿½W	ï¿½ï¿½ï¿½É¯ï¿½ï¿½Õ‘ï¿½Aï¿½tì•ƒï¿½Faï¿½ï¿½H^ï¿½nh8ï¿½ï¿½ï¿½ï¿½e`ï¿½0ï¿½ï¿½É»ï¿½iEï¿½ï¿½<4ï¿½^yï¿½fï¿½aï¿½ï¿½ï¿½ï¿½ï¿½dAØ”(^yEï¿½Uï¿½Â¶2Â—ï¿½$ï¿½ï¿½yhï¿½Xï¿½"Iï¿½Vï¿½ï¿½_cï¿½dBo#eï¿½"$ï¿½ ï¿½ï¿½cï¿½ï¿½ï¿½ï¿½ï¿½\:2ï¿½Sï¿½ï¿½ï¿½7×¶ï¿½ï¿½ï¿½ï¿½Eï¿½U`,ï¿½ï¿½WUï¿½ï¿½Nhï¿½ï¿½ÝŠ

ï¿½Nï¿½l,Yï¿½ï¿½ï¿½ï¿½ï¿½ï¿½bOÛ‰ï¿½ï¿½Vu,|2ï¿½ï¿½ï¿½\ï¿½QfjÆ­.ï¿½ï¿½ÅœUï¿½R\cSï¿½ï¿½ï¿½ï¿½Mï¿½ ï¿½ï¿½x3ï¿½ï¿½yï¿½mCï¿½Tï¿½ï¿½H[<_`ï¿½ï¿½aï¿½ï¿½ï¿½Wï¿½ï¿½ï¿½&ï¿½;ï¿½ï¿½ï¿½LENu6{ï¿½Zï¿½`ï¿½ï¿½QHdpï¿½ï¿½Jï¿½R!hKï¿½<ï¿½ï¿½ï¿½9ï¿½q&>uï¿½oï¿½t\ï¿½ï¿½ï¿½*4ï¿½zï¿½%ï¿½"]]2ï¿½ï¿½]ï¿½Ü‰+ï¿½ï¿½1ï¿½ï¿½ï¿½mï¿½=Kï¿½SG"Cï¿½z(WÕ·ï¿½ï¿½ï¿½tuï¿½?ï¿½ï¿½C=ï¿½2ï¿½5ï¿½7_ï¿½5ï¿½ï¿½ï¿½iï¿½uï¿½ï¿½,ï¿½'Ü¹4ï¿½ï¿½Eï¿½ï¿½oï¿½ï¿½ï¿½ï¿½nï¿½ï¿½ï¿½ï¿½!4ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½

>endobj13011 0 obj<

>stream

Hï¿½\ï¿½ï¿½jï¿½0ï¿½ï¿½z

ï¿½Cï¿½>Jï¿½ï¿½ï¿½ï¿½]hï¿½pï¿½qjï¿½e#;ï¿½}g

O"ï¿½9ï¿½ï¿½ï¿½9Nï¿½3ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½y/ï¿½zï¿½ï¿½ï¿½ï¿½8ï¿½ï¿½)ï¿½M%Nï¿½qRï¿½Oï¿½>=ï¿½ï¿½DOï¿½z@|ï¿½}Aï¿½a=ï¿½ï¿½:Atï¿½ï¿½ï¿½$ï¿½Z(ï¿½Kfï¿½p}ï¿½^G*znï¿½Vï¿½keDï¿½a=Frï¿½e$ï¿½ï¿½\Fï¿½ï¿½~ï¿½ï¿½ï¿½ï¿½ï¿½Qï¿½ï¿½G_Ø›ï¿½ï¿½kï¿½ï¿½ï¿½]ï¿½9|tï¿½8ï¿½ï¿½ï¿½xï¿½_ %Pï¿½ï¿½endstreamendobj13012 0 obj<

>endobj13013 0 obj<

>endobj13014 0 obj<

>endobj13015 0 obj<

>endobj13016 0 obj<

>endobj13017 0 obj<

>stream

Hï¿½\ï¿½ÍŽï¿½0ï¿½}ï¿½ï¿½Ë™ï¿½($ï¿½ï¿½ï¿½Hï¿½aï¿½Ä¢?*ï¿½@bhï¿½ï¿½ï¿½}}|ï¿½ï¿½ï¿½Hï¿½ï¿½ï¿½ï¿½~ï¿½ï¿½ï¿½7ï¿½ï¿½]ï¿½M&ï¿½>^ï¿½}ï¿½Ì©ï¿½ï¿½1Ü®ï¿½ï¿½	ï¿½ï¿½]ï¿½ï¿½iï¿½fï¿½Gé·¹ï¿½,ï¿½ï¿½ï¿½)\vï¿½ï¿½-ï¿½&ï¿½/Þ¦ï¿½aï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ø†ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½fï¿½lï¿½ï¿½}ï¿½ï¿½Kï¿½'ï¿½0ï¿½ï¿½iï¿½)Nï¿½ï¿½0|=\ï¿½ï¿½ï¿½c/ï¿½6^ï¿½ï¿½K|ï¿½óŽŸ!ï¿½2ï¿½bï¿½knÃ¡	ï¿½?ï¿½lï¿½ï¿½ï¿½ï¿½Yï¿½ï¿½ï¿½Bï¿½ï¿½wï¿½ï¿½|ï¿½xj~ï¿½lYlï¿½Í‹ï¿½ï¿½ï¿½ï¿½Vï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½rï¿½ï¿½ï¿½ï¿½eï¿½ï¿½ï¿½,ÈŽï¿½!+3ï¿½*_ï¿½_ï¿½+ï¿½4ï¿½ï¿½2ï¿½%,%mï¿½3]ï¿½3Mï¿½--=[0ï¿½%sï¿½Lï¿½ï¿½ï¿½ï¿½iï¿½tZ8-ï¿½Nï¿½=2ï¿½fKï¿½ï¿½Ù®ï¿½ï¿½ï¿½ï¿½[ï¿½ï¿½ï¿½Ahï¿½ï¿½Ahï¿½ï¿½Ahï¿½ï¿½Ahï¿½ï¿½Ahï¿½Iï¿½Pï¿½Ca=ï¿½Pï¿½Ca=tï¿½Cï¿½~Gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½wï¿½;ï¿½ï¿½~Gï¿½ï¿½ï¿½ï¿½ï¿½ï¿½wï¿½;ï¿½ï¿½~Gï¿½ï¿½_YWQWYWQWYWQWYWQWYWQWuï¿½ï¿½"ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½nï¿½+ï¿½jï¿½Ë¾)ï¿½ï¿½ï¿½ï¿½@ï¿½t^ï¿½ï¿½k}ï¿½y=ï¿½oï¿½}ï¿½è›§ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½{ï¿½=ï¿½ï¿½~ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½{ï¿½=ï¿½ï¿½~ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½bï¿½ï¿½ï¿½*S.pOeï¿½ï¿½.ï¿½0ï¿½]*eï¿½ï¿½ï¿½ï¿½ï¿½Tiï¿½"ï¿½ï¿½wÙ¦Ìº[ï¿½Yï¿½|ï¿½ï¿½5ß«ï¿½<5ï¿½55mï¿½iï¿½yï¿½5ï¿½ï¿½|ï¿½{ï¿½}ã–—ï¿½Ù´ï¿½aï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Ä§ï¿½ï¿½ï¿½

0 ï¿½L=endstreamendobj13018 0 obj<

>endobj13019 0 obj<

>endobj13020 0 obj<

>stream

Hï¿½\ï¿½ï¿½jï¿½0ï¿½ï¿½~

ï¿½Cqï¿½&Ðƒ	ï¿½ï¿½Aï¿½`ï¿½ ï¿½ï¿½Ì°ï¿½ï¿½qyï¿½ï¿½vï¿½`[?!ï¿½ï¿½dï¿½kï¿½ï¿½ï¿½ï¿½{+0iï¿½<ï¿½vï¿½á†³6ï¿½ï¿½@iï¿½Qï¿½ï¿½2:ï¿½I<ï¿½kï¿½ï¿½7ï¿½eB ï¿½ï¿½ï¿½ï¿½ï¿½Nï¿½ï¿½

ï¿½63ï¿½ï¿½ï¿½ï¿½ï¿½9ï¿½ï¿½ï¿½ ï¿½-(ï¿½ï¿½ï¿½ï¿½ï¿½^ï¿½ï¿½'Ù©Wï¿½ï¿½a?ï¿½ï¿½ï¿½swUï¿½ï¿½<ï¿½ï¿½

W7Jï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Lï¿½24ï¿½_ï¿½,ï¿½ï¿½6ï¿½ï¿½ï¿½3Qï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½qï¿½ï¿½#ï¿½ï¿½ï¿½ï¿½sMkï¿½&sï¿½ï¿½ï¿½Bï¿½ï¿½Mï¿½ï¿½Tï¿½ï¿½ï¿½uï¿½:vï¿½ ï¿½H/	ï¿½ï¿½ï¿½ï¿½=ï¿½ï¿½ï¿½;ï¿½ï¿½ï¿½?ï¿½Rï¿½ï¿½~ ï¿½Hï¿½.endstreamendobj

[Content truncated...]

---

## âœ“ Google finds AI chatbots are only 69% accurateâ€¦ at best - Digital Trends

**URL:** https://www.digitaltrends.com/computing/google-finds-ai-chatbots-are-only-69-accurate-at-best/

Google

has published a blunt assessment of how reliable todayâ€™s AI chatbots really are, and the numbers are not flattering. Using its newly introduced

FACTS Benchmark Suite

, the company found that even

the best AI models

struggle to break past a 70% factual accuracy rate. The top performer,

Gemini 3 Pro

, reached 69% overall accuracy, while other leading systems from

OpenAI

,

Anthropic

, and

xAI

scored even lower. The takeaway is simple and uncomfortable. These chatbots still get roughly one out of every three answers wrong,

even when they sound confident doing it

.

The benchmark matters because most existing

AI tests focus on whether a model can complete a task, not whether the information it produces is actually true

. For industries like finance, healthcare, and law, that gap can be costly. A fluent response that sounds confident but contains errors can do real damage, especially when users assume the chatbot knows what it is talking about.

What Googleâ€™s accuracy test reveals

Google

The FACTS Benchmark Suite was built by Googleâ€™s FACTS team with Kaggle to directly test factual accuracy across four real-world use. One test measures parametric knowledge, which checks whether a model can answer fact-based questions using only what it learned during training. Another evaluates search performance, testing how well models use web tools to retrieve accurate information. A third focuses on grounding, meaning whether the model sticks to a provided document without adding false details. The fourth examines multimodal understanding, such as reading charts, diagrams, and images correctly.

Google

The results show sharp differences between models. Gemini 3 Pro led the leaderboard with a 69% FACTS score, followed by Gemini 2.5 Pro and OpenAIâ€™s

ChatGPT-5

nearly at 62% percent. Claude 4.5 Opus landed at ~51% percent, while Grok 4 scored ~54%. Multimodal tasks were the weakest area across the board, with accuracy often below 50%. This matters because these tasks involve reading charts, diagrams, or images, where a chatbot could confidently misread a sales graph or pull the wrong number from a document, leading to mistakes that are easy to miss but hard to undo.

Recommended Videos

The takeaway isnâ€™t that chatbots are useless, but blind trust is risky. Googleâ€™s own data suggests AI is improving, yet it still needs verification, guardrails, and human oversight before it can be treated as a reliable source of truth.

---

## âœ“ How Intelligent Is AI, Really? - YouTube

**URL:** https://www.youtube.com/watch?v=pBlIgs6w7Ss

About

Press

Copyright

Contact us

Creator

Advertise

Developers

Terms

Privacy

Policy & Safety

How YouTube works

Test new features

Â© 2026 Google LLC

---

## âœ“ Reddit - The heart of the internet

**URL:** https://www.reddit.com//r/technology/comments/1lntrgj?utm_source=chatgpt.com

Skip to main content

---


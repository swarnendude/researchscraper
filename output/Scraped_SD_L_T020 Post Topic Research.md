# Scraped Content

Total URLs: 31

---

## ✗ https://medium.com/@sachinsoni600517/introduction-to-rag-retrieval-augmented-generation-and-vector-database-b593e8eb6a94

**URL:** https://medium.com/@sachinsoni600517/introduction-to-rag-retrieval-augmented-generation-and-vector-database-b593e8eb6a94

Error scraping: 403 Client Error: Forbidden for url: https://medium.com/@sachinsoni600517/introduction-to-rag-retrieval-augmented-generation-and-vector-database-b593e8eb6a94

---

## ✓ [2512.01659] HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment

**URL:** https://arxiv.org/abs/2512.01659

Computer Science > Machine Learning

arXiv:2512.01659

(cs)

[Submitted on 1 Dec 2025]

Title:

HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment

Authors:

Valentin Noël

,

Elimane Yassine Seidou

,

Charly Ken Capo-Chichi

,

Ghanem Amari

View a PDF of the paper titled HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment, by Valentin No\"el and Elimane Yassine Seidou and Charly Ken Capo-Chichi and Ghanem Amari

View PDF

HTML (experimental)

Abstract:

Legal AI systems powered by retrieval-augmented generation (RAG) face a critical accountability challenge: when an AI assistant cites case law, statutes, or contractual clauses, practitioners need verifiable guarantees that generated text faithfully represents source documents. Existing hallucination detectors rely on semantic similarity metrics that tolerate entity substitutions, a dangerous failure mode when confusing parties, dates, or legal provisions can have material consequences. We introduce HalluGraph, a graph-theoretic framework that quantifies hallucinations through structural alignment between knowledge graphs extracted from context, query, and response. Our approach produces bounded, interpretable metrics decomposed into \textit{Entity Grounding} (EG), measuring whether entities in the response appear in source documents, and \textit{Relation Preservation} (RP), verifying that asserted relationships are supported by context. On structured control documents, HalluGraph achieves near-perfect discrimination ($>$400 words, $>$20 entities), HalluGraph achieves $AUC = 0.979$, while maintaining robust performance ($AUC \approx 0.89$) on challenging generative legal task, consistently outperforming semantic similarity baselines. The framework provides the transparency and traceability required for high-stakes legal applications, enabling full audit trails from generated assertions back to source passages.

Comments:

8 pages, 4 figures, under review

Subjects:

Machine Learning (cs.LG)

; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)

Cite as:

arXiv:2512.01659

[cs.LG]

(or

arXiv:2512.01659v1

[cs.LG]

for this version)

https://doi.org/10.48550/arXiv.2512.01659

Focus to learn more

arXiv-issued DOI via DataCite

Submission history

From: Valentin Noël [

view email

]

[v1]

Mon, 1 Dec 2025 13:31:06 UTC (12 KB)

Full-text links:

Access Paper:

View a PDF of the paper titled HalluGraph: Auditable Hallucination Detection for Legal RAG Systems via Knowledge Graph Alignment, by Valentin No\"el and Elimane Yassine Seidou and Charly Ken Capo-Chichi and Ghanem Amari

View PDF

HTML (experimental)

TeX Source

view license

Current browse context:

cs.LG

< prev

|

next >

new

|

recent

|

2025-12

Change to browse by:

cs

cs.AI

cs.CL

References & Citations

NASA ADS

Google Scholar

Semantic Scholar

export BibTeX citation

Loading...

BibTeX formatted citation

×

loading...

Data provided by:

Bookmark

Bibliographic Tools

Bibliographic and Citation Tools

Bibliographic Explorer Toggle

Bibliographic Explorer

(

What is the Explorer?

)

Connected Papers Toggle

Connected Papers

(

What is Connected Papers?

)

Litmaps Toggle

Litmaps

(

What is Litmaps?

)

scite.ai Toggle

scite Smart Citations

(

What are Smart Citations?

)

Code, Data, Media

Code, Data and Media Associated with this Article

alphaXiv Toggle

alphaXiv

(

What is alphaXiv?

)

Links to Code Toggle

CatalyzeX Code Finder for Papers

(

What is CatalyzeX?

)

DagsHub Toggle

DagsHub

(

What is DagsHub?

)

GotitPub Toggle

Gotit.pub

(

What is GotitPub?

)

Huggingface Toggle

Hugging Face

(

What is Huggingface?

)

Links to Code Toggle

Papers with Code

(

What is Papers with Code?

)

ScienceCast Toggle

ScienceCast

(

What is ScienceCast?

)

Demos

Demos

Replicate Toggle

Replicate

(

What is Replicate?

)

Spaces Toggle

Hugging Face Spaces

(

What is Spaces?

)

Spaces Toggle

TXYZ.AI

(

What is TXYZ.AI?

)

Related Papers

Recommenders and Search Tools

Link to Influence Flower

Influence Flower

(

What are Influence Flowers?

)

Core recommender toggle

CORE Recommender

(

What is CORE?

)

IArxiv recommender toggle

IArxiv Recommender

(

What is IArxiv?

)

Author

Venue

Institution

Topic

About arXivLabs

arXivLabs: experimental projects with community collaborators

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community?

Learn more about arXivLabs

.

Which authors of this paper are endorsers?

|

Disable MathJax

(

What is MathJax?

)

---

## ✓ Reddit - The heart of the internet

**URL:** https://www.reddit.com/r/Rag/comments/1qed97y/a_user_shared_to_me_this_complete_rag_guide/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button

Go to Rag

r/Rag

•

Real-Turnover9685

A user shared to me this complete RAG guide

Someone juste shared to me this complete RAG guide with everything from parsing to reranking. Really easy to follow through.

Link :

https://app.ailog.fr/en/blog

Read more

Share

---

## ✗ https://medium.com/@hamipirzada/understanding-the-rag-architecture-model-a-deep-dive-into-modern-ai-c81208afa391

**URL:** https://medium.com/@hamipirzada/understanding-the-rag-architecture-model-a-deep-dive-into-modern-ai-c81208afa391

Error scraping: 403 Client Error: Forbidden for url: https://medium.com/@hamipirzada/understanding-the-rag-architecture-model-a-deep-dive-into-modern-ai-c81208afa391

---

## ✓ How RAG Makes AI Trustworthy — No More Hallucinations #RAG #ai - YouTube

**URL:** https://youtube.com/shorts/dM7cJMVPFeA?si=VyMoCQ6ZRppAoOXg

About

Press

Copyright

Contact us

Creator

Advertise

Developers

Terms

Privacy

Policy & Safety

How YouTube works

Test new features

© 2026 Google LLC

---

## ✓ https://x.com/neo4j/status/2012500674643169704?s=20

**URL:** https://x.com/neo4j/status/2012500674643169704?s=20

JavaScript is not available.

We’ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center.

Help Center

Terms of Service

Privacy Policy

Cookie Policy

Imprint

Ads info

© 2026 X Corp.

Something went wrong, but don’t fret — let’s give it another shot.

Try again

Some privacy related extensions may cause issues on x.com. Please disable them and try again.

---

## ✓ Enterprise AI Adoption Trends 2026: What's Driving the Revolution | Hakia

**URL:** https://www.hakia.com/tech-insights/enterprise-ai-adoption/?utm_source=chatgpt.com

Key Takeaways

1

.

87% of enterprises now use AI in production, up from 31% in 2020 (

McKinsey Global Institute

)

2

.

RAG architecture dominates enterprise AI implementations, used by 73% of production LLM systems

3

.

Investment in AI infrastructure reached $89 billion in 2024, with vector databases growing 340% year-over-year

4

.

Cost optimization has become the primary challenge, with 64% of organizations citing budget constraints as their top barrier

Table of Contents

87%

Enterprise AI Adoption

14 months

Average ROI Timeframe

$89B

AI Infrastructure Investment

73%

Production RAG Systems

Enterprise AI Adoption Statistics: The Acceleration Continues

Enterprise AI adoption has reached an inflection point in 2025. According to the

McKinsey Global Institute's latest report

, 87% of enterprises now have at least one AI system in production, representing a dramatic increase from 31% in 2020. This acceleration reflects both technological maturation and competitive pressures driving digital transformation.

The growth is particularly pronounced in specific AI categories.

Natural language processing applications

lead adoption at 76%, followed by computer vision at 59%, and predictive analytics at 54%. Notably,

conversational AI and chatbots

have seen the fastest growth, with deployment rates increasing 312% year-over-year as organizations rush to deploy customer service automation and internal knowledge management systems.

What's driving this rapid adoption? The

Gartner Enterprise AI Survey

identifies three primary factors: pressure to improve operational efficiency (cited by 78% of respondents), competitive differentiation needs (67%), and cost reduction mandates (61%). The COVID-19 pandemic's lasting effects on digital-first business models continue to influence strategic priorities, with remote work and distributed teams creating demand for AI-powered collaboration tools.

312%

Conversational AI Growth

Source:

McKinsey Global Institute 2024

Leading AI Technologies in Enterprise: RAG Dominates Production Systems

Retrieval-Augmented Generation (RAG)

has emerged as the dominant architecture for enterprise LLM implementations. Our analysis shows 73% of production language model systems use RAG to ground responses in company-specific knowledge bases. This preference stems from RAG's ability to provide accurate, source-attributable answers while avoiding the high costs and risks associated with fine-tuning proprietary models.

The technology stack supporting enterprise AI has also crystallized around specific tools and platforms.

Vector databases

have seen explosive growth, with Pinecone, Weaviate, and Chroma leading enterprise deployments. The vector database market grew 340% in 2024, driven primarily by RAG implementations requiring semantic search capabilities.

Large Language Models:

OpenAI GPT-4 leads with 45% market share, followed by Anthropic Claude (23%) and open-source models (32%)

Vector Databases:

Pinecone dominates with 41% of enterprise deployments, Weaviate at 28%, pgvector at 19%

MLOps Platforms:

Databricks (34%), AWS SageMaker (29%), Google Vertex AI (22%)

Model Serving:

NVIDIA Triton (31%), MLflow (26%), TorchServe (21%)

Interestingly, the rise of

AI agents

represents the next frontier. While still early, 23% of enterprises are piloting autonomous AI systems that can take actions beyond text generation. These implementations focus primarily on customer service, data analysis, and software development assistance, with early adopters reporting 35-50% productivity gains in targeted use cases.

RAG (Retrieval-Augmented Generation)

AI architecture combining LLMs with external knowledge retrieval for accurate, grounded responses

Key Skills

Vector databases

Embedding models

Prompt engineering

Common Jobs

•

AI Engineer

•

ML Engineer

•

Backend Developer

Vector Database

Specialized database optimized for storing and querying high-dimensional vector embeddings

Key Skills

Semantic search

Similarity matching

Indexing algorithms

Common Jobs

•

Data Engineer

•

ML Engineer

•

AI Developer

MLOps Platform

Infrastructure for managing the machine learning lifecycle from training to production deployment

Key Skills

Model versioning

Continuous deployment

Monitoring

Common Jobs

•

ML Engineer

•

DevOps Engineer

•

Data Scientist

Implementation Challenges and Solutions: The Reality of Enterprise AI

Despite the enthusiasm for AI adoption, enterprises face significant implementation challenges. The

Deloitte State of AI in Enterprise

survey reveals that cost management has overtaken technical complexity as the primary barrier, with 64% of organizations citing budget constraints as their biggest obstacle.

The cost challenge is multifaceted.

LLM inference costs

can quickly spiral out of control, with some enterprises reporting monthly API bills exceeding $500,000 for production systems. This has driven significant investment in

optimizati

[Content truncated...]

---

## ✓ 5 Techniques to Prevent Hallucinations in Your RAG Question Answering

**URL:** https://eivindkjosbakken.substack.com/p/5-techniques-to-prevent-hallucinations?utm_source=%2Fsearch%2Frag%2520hallucinations&utm_medium=reader2

5 Techniques to Prevent Hallucinations in Your RAG Question Answering

Learn how to reduce the number of hallucinations and the impact they have

Eivind Kjosbakken

Jan 18, 2026

Share

Hallucinations are a massive problem when working with LLMs. They are a problem for two main reasons. The first apparent reason is that a hallucination naturally causes the user to receive an incorrect response. The second, arguably worse reason, is that hallucinations lower the users’ trust in the system. Without the user believing in your question answering system, it will be difficult to keep the users on your platform.

Table of contents

Why do you need to minimize hallucinations?

Techniques to prevent hallucinations

Summary

Why do you need to minimize hallucinations?

When clients think about using an LLM to solve a problem, one of the first thoughts that often comes to mind is hallucinations. They’ve often heard that LLMs sometimes output text that isn’t true, or at least answers you cannot fully trust.

Unfortunately, they are often correct, and you need to take steps to minimize hallucinations in your question answering systems. The contents of this article will refer to hallucinations specifically for question answering systems, though all of the techniques can be applied to other LLM applications as well, such as:

Classification

Information extraction

Automations

Throughout this article, I’ll discuss real-world techniques you can apply to mitigate the impact of hallucinations, either by preventing them from happening at all or by minimizing the damage a hallucination causes. This can, for example, be damage such as the user trusting your application less after experiencing

Upgrade to Paid

Techniques to prevent hallucinations

I’ll separate this section into two subsections:

Techniques to directly lower the amount of hallucinations you experience from the LLMs

Techniques to mitigate the damage of hallucinations

I do this because I think it’s helpful to get an overview of techniques you can utilize to minimize the impact of hallucinations. You can either try to prevent them from happening at all (the first technique), though this is near impossible to achieve with 100% accuracy, or you can mitigate the damage of a hallucination once it happens (technique number two).

Lower the amount of hallucinations

In this subsection, I’ll cover the following techniques:

Verification step (LLM judge)

RAG improvements

Optimize your system prompt

LLM judge verification

The first technique I’ll cover is utilizing LLM as a judge to verify your LLM responses. This technique relies on the concept below:

Verifying a response, is usually a simpler task than generating the response

This quote is sometimes more easily understood for math problems, where coming up with a solution is often rather difficult, but verifying that the solution is correct is a lot simpler. However, this concept also applies to your question answering system.

To generate a response, an LLM has to read through a lot of text and interpret the user’s question. The LLM then has to come up with a suitable response, given the context it’s been fed. Verifying the answer, however, is usually easier, considering the LLM verifier needs to judge whether the final response makes sense given the question and the context. You can read more about LLM validation in my article on

Large Scale LLM Output Validation

.

RAG improvements

There are also a lot of improvements you can make to your RAG pipeline to prevent hallucinations. Step number one is to fetch the correct documents.

I recently wrote about this process

, with techniques to both increase the precision and recall of the documents you fetch for RAG. It mainly boils down to filtering away potentially irrelevant documents (increasing precision) through techniques like reranking and LLM verification. On the other side, you can ensure to include relevant documents (increase recall), with techniques such as contextual retrieval and fetching more document chunks.

This image highlights a traditional RAG pipeline, where the user inputs their query. You embed this query and find the most semantically similar documents through embedding similarity. These documents are fed into an LLM, which provides an answer to the user’s query, given the provided documents. Image by the author.

Get access to the full archive of articles, and first access to webinars and other content I create

Subscribe

Optimize your system prompt

Another technique you can utilize to lower the amount of hallucinations is to improve your system prompt. Anthropic recently wrote an article about writing tools for AI agents, highlighting how they use Claude Code to optimize their prompts. I recommend doing something similar, where you feed all your prompts through an LLM, asking the LLM to improve your prompt in general, and also highlighting cases where your prompt succeeds and where it fails.

Furthermore, you should include a sentence

[Content truncated...]

---

## ✓ RESEARCH@DBTA: Survey: RAG Emerges as the Connective Tissue of Enterprise AI - Database Trends and Applications

**URL:** https://www.dbta.com/Editorial/Trends-and-Applications/RESEARCH-at-DBTA-Survey-RAG-Emerges-as-the-Connective-Tissue-of-Enterprise-AI-167699.aspx

While the generative AI (GenAI) revolution is rolling forward at full steam, it’s not without its share of fear, uncertainty, and doubt.

The great promises that can be delivered through large language models (LLMs) are tainted by concerns over hallucinations, bias, data security, “black-box” decisioning, and outdated information. Enterprises are addressing many of these issues through foundation LLMs as well as their own LLM implementations. Many are also exploring the potential of retrieval-augmented generation (RAG) environments to serve as the connective tissue between corporate databases and LLMs.

These are the key takeaways from a recent survey on LLM and RAG adoption, explored in a survey of 382 executives and managers conducted by Unisphere Research, a service of Information Today, Inc., in partnership with Semantic Web Company, and covering organizations primarily within North America with revenues exceeding $500 million annually.

LLMs are becoming pervasive across most organizations, the survey found. Their use is most pervasive in the testing and development stage. Eighty-five percent of respondents are either exploring and testing their potential or have LLMs in production. At least another 27% already have LLMs in production in some capacity.

Only 7% report no activity at this time. Nine in 10 respondents will keep expanding their LLM implementations.

Plus, 68% rely on outside models, such as OpenAI’s ChatGPT, Claude, or Midjourney. Only a handful of enterprises are relying on their own internal LLM or GenAI services.

There are compelling reasons to move to LLMs to deliver insights. Respondents with LLMs in production are most likely to look for internal productivity benefits from GenAI. A majority, 67%, are seeking to help employees access insights, followed closely by 65% expecting employee productivity gains, and another 65% seeking to reduce the time for knowledge workers to access the information they require. In contrast, the leading benefit cited overall—including those still exploring LLMs—is potential productivity benefits.

At the same time, there is rising concern about risks associated with expanding LLM deployments, the survey shows. More than 7 in 10 respondents, 71%, see their increased usage of GenAI as a risk in terms of security and output. Interestingly, 10% do not see risk, which may also suggest they see AI tools helping to address security issues.

Data quality concerns top the list of issues that organizations face with GenAI and LLM implementations, cited by a majority of respondents at 71%. A majority also see data security and privacy concerns as pressing challenges. Respondents with LLMs in production, 89%, agree, almost unanimously, that it’s important to some degree to have a human in the loop for their GenAI and LLM systems.

Close to a third of GenAI users are looking to RAG environments to support their information handling. More than 1 in 4 respondents at current LLM/AI sites, 29%, report that they either have RAG solutions in place or are implementing these solutions. Most agree that their businesses will depend on it for not just technical capability, but for competitive advantage.

Close to half agree that RAG will help make information more actionable and closer to real time.

This is how one survey respondent put it: RAG “helps by making AI smarter and efficient. It does so by connecting AI with other organizations’ unique data, which supports these systems to generate responses that are both more accurate and more contextually relevant.”

RAG environments retrieve and store data in a variety of sources, domains, and formats. Relational databases, knowledge graphs, and vector databases compete for the top database technologies that are interfacing with RAG implementations.

Benefits anticipated through RAG-empowered AI implementations include improved contextual results, more actionable data, and reduced time to insight, cited by a close to a majority of respondents now using AI at 48%.

Generative AI and RAG technologies are still both in the early stages of their deployments. Most report their efforts are still immature, with applications within the testing and development stage.

Still, enthusiasm is prevalent. GenAI and LLMs are expanding across most of the organizations participating in this survey, with the majority employing or testing AI to enhance the delivery of knowledge across their enterprises, especially within content creation, content customization, customer self-service, knowledge discovery, knowledge management, intelligent search, and assisting customer service staff.

---

## ✓ Reddit - The heart of the internet

**URL:** https://www.reddit.com/r/Rag/comments/1otnphx/built_rag_systems_with_10_tools_heres_what/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button

Go to Rag

r/Rag

•

Otherwise_Flan7339

Built RAG systems with 10+ tools - here's what actually works for production pipelines

Spent the last year building RAG pipelines across different projects. Tested most of the popular tools - here's what works well for different use cases.

Vector stores:

Chroma

- Open-source, easy to integrate, good for prototyping. Python/JS SDKs with metadata filtering.

Pinecone

- Managed, scales well, hybrid search support. Best for production when you need serverless scaling.

Faiss

- Fast similarity search, GPU-accelerated, handles billion-scale datasets. More setup but performance is unmatched.

Frameworks:

LangChain

- Modular components for retrieval chains, agent orchestration, extensive integrations. Good for complex multi-step workflows.

LlamaIndex

- Strong document parsing and chunking. Better for enterprise docs with complex structures.

LLM APIs:

OpenAI

- GPT-4 for generation, function calling works well. Structured outputs help.

Google Gemini

- Multimodal support (text/image/video), long context handling.

Evaluation/monitoring:

RAG pipelines fail silently in production. Context relevance degrades, retrieval quality drops, but users just get bad answers.

Maxim's RAG evaluation

tracks retrieval quality, context precision, and faithfulness metrics. Real-time observability catches issues early without affecting large audience .

MongoDB Atlas

is underrated - combines NoSQL storage with vector search. One database for both structured data and embeddings.

The biggest gap in most RAG stacks is evaluation. You need automated metrics for context relevance, retrieval quality, and faithfulness - not just end-to-end accuracy.

What's your RAG stack? Any tools I missed that work well?

Read more

Share

---

## ✓ The Science Behind RAG: How It Reduces AI Hallucinations

**URL:** https://zerogravitymarketing.com/blog/the-science-behind-rag

The Science Behind RAG & How It Reduces AI Hallucinations

Apr 25

|

AI

,

SEO

|

Tyler

Bishop

What is Retrieval-Augmented Generation (RAG) in AI?

One of the biggest challenges developers and users face in the world of artificial intelligence is dealing with “hallucinations” – moments when AI models produce confident-sounding but completely inaccurate information. While these moments are amusing in casual chats, this issue can pose a serious risk in professional or sensitive contexts.

Retrieval-Augmented Generation (RAG)

– a powerful architectural approach that aims to significantly reduce these hallucinations. RAG does this by allowing AI models to access external data sources at the time of generating a response. Instead of relying solely on the information encoded during the training period, RAG models retrieve real-time, relevant information, grounding their outputs in verifiable facts.

This dynamic makes RAG more reliable and much more adaptable to real-world use cases, particularly in industries like healthcare, law, and enterprise support, where accuracy matters most.

Overview of RAG Architecture

At its core, RAG is a hybrid system that marries two key components: a retriever and a generator. The retriever’s job is to scan a knowledge base and pull the most relevant documents or data snippets based on the user’s query. Then, the generator – typically a large language model (LLM) like GPT – synthesizes this information into a coherent,

human-readable response.

What sets this apart from traditional AI models? The generator isn’t working in isolation. Instead of relying solely on its internal knowledge, which may be outdated or incomplete, it’s being fed fresh, relevant data on the fly. This architecture

empowers AI

to produce outputs that are not only plausible-sounding but grounded in reality.

How RAG Differs from Traditional AI Models

Traditional generative models operate like a closed book – they rely entirely on the data they’ve been trained on. While that training can be extensive, it’s also static. Once trained, the model’s knowledge is frozen in time. Updating it requires re-training or fine-tuning, which is often resource-intensive and slow.

But RAG changes the game by enabling AI to “look things up” in real time. Rather than guessing or extrapolating based on old,

outdated knowledge

, the model fetches current and contextually relevant data to guide its responses. This makes RAG far more scalable, especially for applications that require frequently updated or domain-specific information.

Comparison with Fine-Tuning and RLHF

While fine-tuning and Reinforcement Learning with Human Feedback (RLHF) have been essential in aligning language models with human preferences and values, they don’t solve the hallucination issue. Fine-tuning content outputs helps inject new knowledge, but the process is time-consuming. RLHF improves the quality of responses based on human feedback, but it doesn’t change the fact that the model’s information is still limited to what it was trained on.

RAG, by contrast, bypasses these limitations. It doesn’t need a training overhaul to access new data – it simply retrieves it from an external source. That makes RAG an elegant, cost-effective solution for maintaining up-to-date, trustworthy AI outputs.

How RAG Works

Key Components: Retriever and Generator

The brilliance of RAG lies in how the retriever and generator work together. The retriever is designed to search across a vast collection of documents – whether it’s internal knowledge bases, product manuals, or scientific research – to find the most semantically relevant pieces of information. It uses vector embeddings and semantic search to understand not just keywords, but the meaning behind queries.

Once those relevant chunks are identified, the generator takes over. This is typically a pre-trained LLM that knows how to take retrieved text and generate a response that is coherent, informative, and human-like. The result is a dynamic, real-time synthesis of stored knowledge and generative capability.

Integration of Structured and Unstructured Data

One of RAG’s strengths is its flexibility in dealing with both

structured and unstructured data

. While many AI systems struggle to make sense of unstructured formats – like PDF documents, blog posts, or conversation transcripts – RAG can understand these just as effectively as it does structured databases or spreadsheets.

This makes it especially helpful for enterprises where critical knowledge often lives in different formats. Whether it’s CRM logs, technical wikis, or customer support transcripts, RAG can pull relevant context from all of them – no data is left behind.

Utilization of Vector Databases and Semantic Search

The retriever component often relies on vector databases – such as Pinecone, FAISS, or Weaviate – to perform fast, accurate semantic search. These systems convert both the user query and the stored documents into high-dimensio

[Content truncated...]

---

## ✓ Reddit - The heart of the internet

**URL:** https://www.reddit.com/r/LocalLLaMA/comments/1p7781y/need_advice_on_a_highly_accurate_rag_pipeline_for/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button

Go to LocalLLaMA

r/LocalLLaMA

•

VitaminnCPP

हिन्दी

Italiano

ไทย

Need advice on a highly accurate RAG pipeline for massive technical docs (10k–50k pages).

I’m building a RAG system to answer questions from extremely dense technical documentation (think ARM architecture manuals, protocol specs, engineering procedures). Accuracy is more important than creativity. Hallucinations are unacceptable.

Core problems

Simple chunking breaks context; headings, definitions, tables get separated.

Tables, encodings, and instruction formats embed poorly.

Pure vector search fails on exact tokens, opcodes, field names.

Need a backend that supports structure, metadata, and relational links.

Proposed approach (looking for feedback)

Structured extraction

: Convert the entire doc into hierarchical JSON (sections, subsections, definitions, tables, code blocks).

Multi-resolution chunking

:

micro (100–300 tokens: instruction fields, table rows)

mid (400–800 tokens: full sections)

macro (1k–4k tokens: chapters)

Hybrid retrieval

:

Lexical (BM25/FTS) for exact matches

Vector DB for semantic

Cross-encoder/LLM rerank

Separate storage

for tables, constraints, opcode fields, formats.

DB options I’m evaluating

Graph DB

(Neo4j/Arango) for cross-references and hierarchy

SQL

(PostgreSQL) for tables and structured fields

Document store

(Mongo/JSONB) for irregular sections

Likely end result: hybrid stack (SQL + vector DB + FTS), optional graph.

What I need from the community

Is this multi-resolution + hybrid search architecture the right way for highly technical RAG?

Anyone running similar pipelines on local LLMs?

Do I actually need a graph DB, or is SQL + FTS enough?

Best local embedding models for terse technical text?

Looking for architectural critiques, war stories, or DB recommendations from people who’ve built similar systems.

Read more

Share

---

## ✓ Reddit - The heart of the internet

**URL:** https://www.reddit.com/r/LocalLLaMA/comments/1ps6txq/rag_that_actually_works/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button

Go to LocalLLaMA

r/LocalLLaMA

•

TheGlobinKing

हिन्दी

Deutsch

ไทย

Español (Latinoamérica)

RAG that actually works?

When I discovered AnythingLLM I thought I could finally create a "knowledge base" for my own use, basically like an expert of a specific field (e.g. engineering, medicine, etc.) I'm not a developer, just a regular user, and AnythingLLM makes this quite easy. I paired it with llama.cpp, added my documents and started to chat.

However, I noticed poor results from all llms I've tried, granite, qwen, gemma, etc. When I finally asked about a specific topic mentioned in a very long pdf included in my rag "library", it said it couldn't find any mention of that topic anywhere. It seems only part of the available data is actually considered when answering (again, I'm not an expert.) I noticed a few other similar reports from redditors, so it wasn't just matter of using a different model.

Back to my question... is there an easy to use RAG system that "understands" large libraries of complex texts?

Read more

Share

---

## ✓ Retrieval-Augmented Generation (RAG) | Pinecone

**URL:** https://www.pinecone.io/learn/retrieval-augmented-generation/

←

Learn

Retrieval-Augmented Generation (RAG)

Jenna Pederson

Jun 12, 2025

Core Components

Share:

Jump to section:

Limitations of foundation models

What is Retrieval-Augmented Generation?

How does Retrieval-Augmented Generation work?

Wrapping up

Not only are foundation models stuck in the past, but they intentionally produce natural-sounding and varied responses. Both of these can lead to confidently inaccurate and irrelevant output. This behavior is known as “hallucination.”

In this article, we’ll explore the limitations of foundation models and how retrieval-augmented generation (RAG) can address these limitations so chat, search, and agentic workflows can all benefit.

Limitations of foundation models

Products built on top of foundation models alone are brilliant yet flawed as foundation models have multiple limitations:

Knowledge cutoffs

When you ask current models about recent events – like asking about last week’s NBA basketball game or how to use features in the latest iPhone model - they may confidently provide outdated or completely fabricated information, the hallucinations we mentioned earlier.

Models are trained on massive datasets containing years of human knowledge and creative output from code repositories, books, websites, conversations, scientific papers, and more. But after a model is trained, this data is frozen at a specific point in time, the “cutoff”. This cutoff creates a

knowledge gap

, leading them to generate plausible but incorrect responses when asked about recent developments.

Lack depth in domain-specific knowledge

Foundation models have broad knowledge, but can lack depth in specialized domains. High quality datasets might not exist publicly for a domain, not necessarily because they are private, but because they are highly specialized. Consider a medical model that knows about anatomy, disease, and surgical techniques, but struggles with rare genetic conditions and cutting edge therapies. This data might exist publicly to be used during training, but it may not appear enough to train the model correctly. It also requires expert-level knowledge during the training process to contextualize the information.

This limitation can result in responses that are incomplete or irrelevant.

Lack private or proprietary data

In the case of general-purpose, public models, the data (

your

data) does not exist publicly and is inaccessible during training. This means that models don’t know the specifics of your business, whether that be internal company processes and policies, personnel data or email communications, or even the trade secrets of your company. And for good reason: if this data had been included in the training, anyone using the model would potentially gain access to your company’s private and proprietary data.

Again, this limitation can result in incomplete or irrelevant responses, limiting the usefulness of the model for your custom business purpose.

Loses trust

Models typically cannot cite their sources related to a specific response. Without citations or references, the user either has to trust the response or validate the claim themselves. Given that models are trained on vast amounts of public data, there is a chance that the generated response is the result of an unauthoritative source.

When inaccurate, irrelevant, and useless information is generated, users will lose trust in the model itself, even when this behavior is inherent in how foundation models work.

Output generation is probabilistic

Hallucinations are often a symptom of the limitations just described. However, models are trained on a diverse set of data that can contain contradictions, errors, and ambiguous data (in addition to the correct data). Because of this, models assign probabilities to all possible continuations, even the wrong ones. Because of sampling randomness like temperature and top k combined with how a user constructs a prompt (maybe it’s too vague or contains misleading context), models may choose the wrong continuation. The result is output that can contain hallucinations.

Additionally, models don’t always distinguish between what they know vs what they don’t know, sounding confident even when incomplete, inaccurate, or irrelevant. Hallucinations can produce unwanted behaviors and even be dangerous. For example, an inaccurate but highly convincing medical report could lead to life-threatening treatments or no treatment at all.

These foundation model limitations can impact your business bottom line and erode the trust of your users. Retrieval-augmented generation can address these limitations.

What is Retrieval-Augmented Generation?

Retrieval-augmented generation, or RAG, is a technique that uses authoritative, external data to improve the accuracy, relevancy, and usefulness of a model’s output. It does this through the following four core components, which we’ll cover in more detail later in this article:

Ingestion: authoritative data like company proprietary 

[Content truncated...]

---

## ✓ Is it crazy to launch a new product against 100+ existing competitors? - Indie Hackers

**URL:** https://www.indiehackers.com/post/4a5fe58e9c

On August 1st, after a month of development, I launched a product in a niche with more than 100 competitors. Here's the story of how I chose this niche, why I think it's a good idea, and how much money I spent on the launch.

To give you a better understanding of my ideas, here's a brief introduction about myself and my experience. I’m a tech founder with 15 years of experience building B2C and B2B products. Since 2008, I've built over 20 products and launched five companies. The last one was a B2B SaaS with $X million in ARR, which was acquired about a year ago.

The Journey to a New Product

After that exit, I was looking for an interesting and challenging problem worth solving in the AI field. My first attempt was an AI support agent for my friend’s SaaS product. That failed due to the specific nature of customer support queries, but it introduced me to Retrieval-Augmented Generation (RAG).

Understanding the complexities of RAG, I realized that a ready-to-use, out-of-the-box RAG solution could be a valuable tool. Many AI developers and assistant builders likely face the same problems. My second attempt involved creating such a tool, but we struggled to find customers. AI developers prefer open-source libraries, while small and medium businesses need ready-made solutions, not just tools.

During my research, I found that startups in this field had varying success. One reached $3,800 in MRR after four months, while another had only 10-20 customers with a huge product. The market demand didn't seem very high.

Finding the Right Niche

I decided to switch to end-user business solutions instead of tools. Over the next few weeks, I analyzed different niches, focusing on AI B2B SaaS for SMBs. To narrow down my research, I built a list of criteria:

AI solution with RAG as a main component.

B2B SaaS for SMBs.

A new and growing niche (most competitors founded in 2023 or later).

At least 10-15 profitable competitors with no market leader.

The ability to build an MVP in one month.

After some research, I found the "ChatGPT assistant for a website" niche with more than 100 direct competitors and 100+ indirect ones. The top 10 leading solutions were founded in early 2023, showing sustainable growth (verified with SimilarWeb). This intrigued me.

I created a table of competitors with columns for their website, main message, secondary message, use cases, traffic (SimilarWeb), foundation date, and pricing. Google, Product Hunt, and SimilarWeb were immensely helpful.

I identified about 50 products and thoroughly reviewed each website, analyzing their messages, features, pricing models, and traffic. I also read articles and blog posts about the founding and revenue of one of the first products in the niche.

Initially, I felt frustrated because the top products had already achieved $2-3 million in ARR. After 1-1.5 years of development and promotion, they seemed far ahead. The idea of entering a highly competitive niche started to seem crazy.

The Decision to Compete

However, my experience told me that if so many competitors are making money, it indicates high market demand and a growing niche. It also meant some top competitors had already found Product/Market fit, allowing me to bypass this challenging step.

In my previous company, it took me about three years to find Product/Market fit, a tough period in my life. I almost gave up but eventually found the right market, audience, and product. Since that, I'd rather compete with 100+ companies than struggle to find P/M fit again.

On the other hand, entering a highly competitive market isn't simple. You need to identify suboptimal areas, flaws, and limitations to improve. To find these, I tried the most popular solutions in the field to see if there was room for one more tool—mine.

I spent several days mapping competitors' interfaces on a Miro board. This detailed analysis gave me a deep understanding of their features, pros, and cons. I also identified which ones were copies of others.

Launching the Product

The most important discovery was my belief that I could do better. I found many areas in marketing, onboarding, and product development that I could improve. I also envisioned how this product could become something big.

Following my criteria, we started the project on July 1st with a deadline of August 1st, 2024. Working about 10 hours a day without a day off, we managed to launch the product one day before the deadline—July 31st. I created a storymap and then put together a backlog from it for the first release of the product.

You can see what we've built in 30 days at

ordemio.com

. The next major update is in three days, so check out the initial MVP.

Website on Webflow built with a paid template:

We created:

A one-page marketing website.

The first version of the product with a RAG engine.

Pricing and payment options.

Both the marketing website and the product were built as if it were a proven idea with strong P/M fit. So, I can't call it an MVP 

[Content truncated...]

---

## ✓ Enhancing RAG-Retrieval to Improve LLMs Robustness and Resilience to Hallucinations | Springer Nature Link

**URL:** https://link.springer.com/chapter/10.1007/978-3-031-74186-9_17

Abstract

The use of Retrieval-Augmented Generation (RAG) models has become a powerful method for generating natural language by incorporating information retrieval. However, the current retrieval phase of RAG models is somewhat simplistic and has several vulnerabilities that affect the quality and relevance of generated responses. One critical issue is determining the distance between the embedded query vector and the external knowledge vectors, alongside the challenge of selecting an optimal number of relevant retrieved documents. The objective of this study is to boost the strength and resilience of LLMs to hallucinations by refining the RAG retrieval process and, consequently, improving the quality of the responses generated by RAG systems for applications like AI-driven customer support, information retrieval, and interactive AI systems with more accurate and contextually appropriate responses. Experiments with different numbers of retrieved documents were conducted to enhance answer completeness and implement a threshold mechanism to use only high-confidence contexts during the generation phase. The results of our experiments demonstrate that these enhancements lead to more accurate and relevant responses in RAG models. By increasing the number of retrieved documents, the Relevancy of Answers improves by 16% for GPT-4, while their Correctness shows slight improvements of 4% for GPT-4 and 2% for GPT-3.5-Turbo. Furthermore, our approach is compared with the AutoRAG framework and overall surpasses it in terms of efficiency and response quality.

This is a preview of subscription content,

log in via an institution

to check access.

Access this chapter

Log in via an institution

Subscribe and save

Springer+

from €37.37 /Month

Starting from 10 chapters or articles per month

Access and download chapters and articles from more than 300k books and 2,500 journals

Cancel anytime

View plans

Buy Now

Chapter

EUR 29.95

Price includes VAT (India)

Available as PDF

Read on any device

Instant download

Own it forever

Buy Chapter

eBook

EUR 102.71

Price includes VAT (India)

Available as EPUB and PDF

Read on any device

Instant download

Own it forever

Buy eBook

Softcover Book

EUR 119.99

Price excludes VAT (India)

Compact, lightweight edition

Dispatched in 3 to 5 business days

Free shipping worldwide -

see info

Buy Softcover Book

Tax calculation will be finalised at checkout

Purchases are for personal use only

Institutional subscriptions

Similar content being viewed by others

Enhancing Retrieval-Augmented Generation Models with Knowledge Graphs: Innovative Practices Through a Dual-Pathway Approach

Chapter

© 2024

TravQuery: A Customer Support Chatbot Based on Retrieval Augmented Generation (RAG)

Chapter

© 2026

Exploring the Benefits of Iterative Retrieval-Augmented Generation for Risk Mitigation in LLM Responses

Chapter

© 2026

Explore related subjects

Discover the latest articles, books and news in related subjects, suggested using machine learning.

Literature mining

Internetpsychology

Intelligence Augmentation

Lead Optimization

Targeted resequencing

Information Storage and Retrieval

Notes

1.

https://huggingface.co/data sets/jamescalam/llama-2-arxiv-papers-chunked

.

2.

https://openai.com/index/new-and-improved-embedding-model/

.

3.

https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/semantic-search.ipynb

.

4.

https://huggingface.co/data sets/hanyueshf/ml-arxiv-papers-qa

.

5.

https://towardsdatascience.com/evaluating-rag-applications-with-ragas-81d67b0ee31a

.

6.

https://optuna.readthedocs.io/en/stable/

.

References

Augenstein, I., et al.: Factuality challenges in the era of large language models. arXiv preprint

arXiv:2310.05189

(2023)

Brown, T., et al.: Language models are few-shot learners. Adv. Neural. Inf. Process. Syst.

33

, 1877–1901 (2020)

Google Scholar

Cahyawijaya, S., Lovenia, H., Fung, P.: Llms are few-shot in-context low-resource language learners. arXiv preprint

arXiv:2403.16512

(2024)

Chen, G., Yu, W., Sha, L.: Unlocking multi-view insights in knowledge-dense retrieval-augmented generation. arXiv preprint

arXiv:2404.12879

(2024)

Eibich, M., Nagpal, S., Fred-Ojala, A.: Aragog: Advanced rag output grading. arXiv preprint

arXiv:2404.01037

(2024)

Es, S., James, J., Espinosa-Anke, L., Schockaert, S.: Ragas: Automated evaluation of retrieval augmented generation. arXiv preprint

arXiv:2309.15217

(2023)

Fu, J., et al.: Autorag-hp: Automatic online hyper-parameter tuning for retrieval-augmented generation (2024).

https://arxiv.org/abs/2406.19251

Gao, Y., et al.: Retrieval-augmented generation for large language models: A survey. arXiv preprint

arXiv:2312.10997

(2023)

Hambarde, K.A., Proença, H.: Information retrieval: recent advances and beyond. IEEE Access

11

, 76581–76604 (2023).

https://doi.org/10.1109/ACCESS.2023.3295776

Article

Google Scholar

Hu, N., et al.: Benchmarking large language mo

[Content truncated...]

---

## ✗ https://medium.com/@DevBoostLab/graphrag-biggest-upgrade-ai-development-2026-33366891525d

**URL:** https://medium.com/@DevBoostLab/graphrag-biggest-upgrade-ai-development-2026-33366891525d

Error scraping: 403 Client Error: Forbidden for url: https://medium.com/@DevBoostLab/graphrag-biggest-upgrade-ai-development-2026-33366891525d

---

## ✓ Reddit - The heart of the internet

**URL:** https://www.reddit.com/answers/538792e9-b850-43bf-a0b2-c96c8de050af/?q=rag+implementation&source=SERP&upstreamCID=8a99a8c1-3b04-44b4-81ea-96f0ba91f929&upstreamIID=39e865db-f076-4172-ac63-453128ed8e85&upstreamQ=rag+implementation&upstreamQID=b7d16a0a-0ac8-42a2-982d-74cdf1c9b27d

Reddit Rules

Privacy Policy

User Agreement

Accessibility

Reddit, Inc. © 2026. All rights reserved.

---

## ✗ https://www.gartner.com/en/documents/5653423?utm_source=chatgpt.com

**URL:** https://www.gartner.com/en/documents/5653423?utm_source=chatgpt.com

Error scraping: 403 Client Error: Forbidden for url: https://www.gartner.com/en/documents/5653423?utm_source=chatgpt.com

---

## ✓ Make Your RAG Agents Actually Work! (No More Hallucinations) - YouTube

**URL:** https://youtu.be/OejuvdyN_U8?si=Uuvq3xMo73c8x16Z

About

Press

Copyright

Contact us

Creator

Advertise

Developers

Terms

Privacy

Policy & Safety

How YouTube works

Test new features

© 2026 Google LLC

---

## ✓ Jasleen Singh | From Hallucination to Accuracy: Evaluating Context Retrieval in RAG Systems - YouTube

**URL:** https://youtu.be/-JC8rXNUH8Q?si=Zz06j-sMqk4i3G5p

About

Press

Copyright

Contact us

Creator

Advertise

Developers

Terms

Privacy

Policy & Safety

How YouTube works

Test new features

© 2026 Google LLC

---

## ✓ 25 Types of RAG Architectures | Chandra Sekhar | 105 comments

**URL:** https://www.linkedin.com/posts/v-chandra-sekhar_25-types-of-rag-architectures-activity-7418848578753974272-Hb1c?utm_source=share&utm_medium=member_desktop&rcm=ACoAAFUwTzcBuU2jtP2EXlJuXopOmcI8QfjIf08

Chandra Sekhar

1w

Report this post

𝐑𝐀𝐆 𝐢𝐬𝐧’𝐭 𝐚 𝐬𝐢𝐧𝐠𝐥𝐞 𝐩𝐚𝐭𝐭𝐞𝐫𝐧 — 𝐢𝐭’𝐬 𝐚 𝐰𝐡𝐨𝐥𝐞 𝐝𝐞𝐬𝐢𝐠𝐧 𝐬𝐩𝐚𝐜𝐞.

Many people stop at “retrieve some data, then generate an answer.”

That’s the basic version. Real-world RAG systems go much deeper.

Today, we have purpose-built RAG variants such as:

🔹 Self-RAG — the model reviews and improves its own answers

🔹 Corrective RAG — catches mistakes and fixes them mid-response

🔹 Graph RAG — pulls insights from structured knowledge graphs

🔹 Agentic RAG — coordinates agents to solve problems step by step

…and 21 other architectures, each designed for different constraints, data types, and business goals.

In this guide, I’ve unpacked all 25 RAG approaches 👇

✔️ What each architecture actually does

✔️ Where it fits best in real systems

✔️ A simple view of how the workflow runs

👉 Swipe through to explore the full RAG landscape.

552

105 Comments

Like

Comment

Share

Copy

LinkedIn

Facebook

X

Nageena -

1w

Report this comment

Well explained

Like

Reply

2 Reactions

3 Reactions

Hasnat Ahmad

1w

Report this comment

Very well and detailed explanation

Like

Reply

3 Reactions

4 Reactions

Vipin Tiwari

1w

Report this comment

Very well done

Chandra Sekhar

Appreciate the helpful post 👍

Like

Reply

2 Reactions

3 Reactions

Aditya Kushwaha

1w

Report this comment

Well Explained

Like

Reply

1 Reaction

2 Reactions

Harshal Chauhan

1w

Report this comment

This is gold. Too many teams oversimplify RAG and then blame the model. Love how you frame it as purpose-built architectures for real constraints. Definitely bookmarking this 👌

Like

Reply

5 Reactions

6 Reactions

Rakshita Belwal

1w

Report this comment

RAG isn’t one pattern.

It’s a design space and choosing the right one is the real skill.

Like

Reply

1 Reaction

2 Reactions

Sumaiya .

1w

Report this comment

Love the emphasis on intent and constraints. Graph and Agentic RAG especially show how retrieval can evolve beyond simple document search.

Like

Reply

3 Reactions

4 Reactions

Pulkit Tyagi

1w

Report this comment

I can totally resonate with this, really a great share

Like

Reply

1 Reaction

2 Reactions

See more comments

To view or add a comment,

sign in

---

## ✗ https://www.quora.com/What-is-RAG-retrieval-augmented-generation-1/answer/Digitology-Marketing

**URL:** https://www.quora.com/What-is-RAG-retrieval-augmented-generation-1/answer/Digitology-Marketing

Error scraping: 403 Client Error: Forbidden for url: https://www.quora.com/What-is-RAG-retrieval-augmented-generation-1/answer/Digitology-Marketing

---

## ✓ Reddit - The heart of the internet

**URL:** https://www.reddit.com/r/Rag/comments/1nihlmz/best_ways_to_evaluate_rag_implementation/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button

Go to Rag

r/Rag

•

Ir3li4

Português (Brasil)

Français

Deutsch

Italiano

ไทย

Best ways to evaluate rag implementation?

Hi everyone! Recently got into this RAG world and I'm thinking about what are the best practices to evaluate my implementation.

For a bit more of context, I'm working on a M&A startup, we have a database (mongodb) with over 5M documents, and we want to allow our users to ask questions about our documents using NLP.

Since it was only a MVP, and my first project related to RAG, and AI in general, I just followed the LangChain tutorial most of the time, adopting hybrid search and parent / children documents techniques.

The only thing that concerns me the most is retrieval performance, since, sometimes when testing locally, the hybrid search takes 20 sec or more.

Anyways, what are your thoughts? Any tips? Thanks!

Read more

Share

---

## ✗ https://pmc.ncbi.nlm.nih.gov/articles/PMC12540348/

**URL:** https://pmc.ncbi.nlm.nih.gov/articles/PMC12540348/

Error scraping: 403 Client Error: Forbidden for url: https://pmc.ncbi.nlm.nih.gov/articles/PMC12540348/

---

## ✗ https://www.producthunt.com/topics/artificial-intelligence

**URL:** https://www.producthunt.com/topics/artificial-intelligence

Error scraping: 403 Client Error: Forbidden for url: https://www.producthunt.com/topics/artificial-intelligence

---

## ✓ Enterprise RAG Predictions for 2025

**URL:** https://www.vectara.com/blog/top-enterprise-rag-predictions

Source: Deloitte’s State of Gen AI Q3

2024: The year Enterprise RAG graduated from experimental

Looking back,

Gartner

predicted in 2022 that by 2024, the adoption of hyper-automation would reach around 65% of enterprises. In 2023,

Deloitte

said that by 2024 enterprises’ spending on GenAI would grow by 30%.

In the first half of 2024, we saw first-hand how enterprises moved from experimentation to production deployment of Gen AI use cases, and in a very short time. This in spite of the ongoing and sometimes lively debate on “how” - how to make it scalable and how to make it safe. According to Deloitte’s

recent Gen AI survey

, 42% of organizations are seeing significant gains in productivity, efficiency, and cost. In the second half of 2024, the question changed to: what’s next? How do we get from incremental and enterprise citizen gains to real transformative industry use cases? How do we do Gen AI that meets enterprises’ needs, and specifically over their own data? How do we reach real ROI? And how do we measure this?

Although the journey continues, 2024 has fortunately given us some answers. Enterprises are choosing Retrieval Augmented Generation (RAG) for 30-60% of their use cases. RAG comes into play whenever the use case demands high accuracy, transparency, and reliable outputs — particularly when the enterprise wants to use its own or custom data. RAG's abilities to reduce hallucinations, provide explainability and transparency, and ensure the security and privacy of enterprise data sets have made RAG emerge as a standard.

There’s been a remarkable evolution over the last 12 months. The rise of ‘Enterprise RAG’ combined with the significantly enhanced capabilities of models (e.g., GPT-4o, Gemini-2.0, Llama-3.3, and Anthropic-3.5), to enable organizations to move from incremental, internal use cases to ROI-impacting use cases, and scale them into production.

Quick recap: 6 main improvements to RAG in 2024

Much of the readiness to start moving into production with more sophisticated use cases is thanks to the improvements seen in RAG itself over the past year. In 2024, we’ve seen it:

→ Get faster:

LLMs have, in general, become 7x faster. This increased speed allows for a better end-user experience and application response times, leading to new internal and external business opportunities with better customer experiences.

→ Become more economical:

Platforms like Vectara that provide end to end RAG out of the box and reduce workload and maintenance cost, and simplify the mere fact that DIY RAG will most likely involve 20+ APIs and 5-10 vendors to manage.

→ Allow larger contexts with higher accuracy:

LLMs in general have extended support for longer context. This enables the LLM to use more facts, and longer chunks in the generation step, resulting in more accurate responses, with less risk for generation of answers not based in facts.

→ Use your data, in your environment:

Leading enterprise RAG platforms have moved to offer on-premises or in-your-VPC deployment, ensuring that your data never leaves your environment. This helps companies address both the safety and security of their data as well as the data gravity of where their workloads already exist.

→ Lower hallucination rates:

LLMs continue to demonstrate lower intrinsic

hallucination rates

, while fast and effective hallucination evaluation models like Vectara’s

HHEM

model have become available to help enterprises prevent incorrect answers from reaching their application consumers.

→ Scale:

As RAG adoption moved from PoC to production, it became increasingly clear that the “R” in RAG (i.e. Retrieval) is one of the biggest bottlenecks. In large enterprise production deployments, the grounding dataset is often quite large, and retrieving the most useful facts for the generation step becomes a challenge. More mature enterprise RAG platforms now integrate flexible and robust search techniques like

hybrid search

and

reranking

to address these scale challenges.

Top 7 predictions for RAG in 2025

So where does enterprise RAG evolve from here? Here are our predictions for 2025:

RAG platforms will become de facto

when approaching DIY

Enterprises have realized the costs and risks associated with going down the DIY path alone and want to avoid wasting sparse resources stitching all the required components together themselves. Instead, they will turn to mature enterprise RAG platform vendors and implementation partners to support the journey.

The fight against hallucinations continues

For RAG in general but with more intense focus as Agentic AI is on the rise (as mistakes will have more significant ripple effects in a chain of actions), it will become even more important to address hallucinations. More innovation will hence emerge around building frontier models that hallucinate less by design, as well as helper models analyzing input or output.

The quality of RAG responses will continue to improve

Better data parsing and p

[Content truncated...]

---

## ✓ Episode 2: Contextualizing Data with RAG | Solving AI Hallucinations in ERPNext

**URL:** https://claudion.substack.com/p/episode-2-contextualizing-data-with?utm_source=%2Fsearch%2Frag%2520hallucinations&utm_medium=reader2

Playback speed

×

Share post

Share post at current time

Share from 0:00

0:00

/

0:00

1

Episode 2: Contextualizing Data with RAG | Solving AI Hallucinations in ERPNext

Claudion

Jan 14, 2026

1

Share

In this second episode of our AI series, we move beyond the initial server setup to explore the critical role of data preparation and contextualization.

We discuss why simply installing a model like Qwen is not enough to handle the complex requirements of an ERP system.

The Mechanics of Next-Word Prediction

Large Language Models function by finding the most probable next word or letter in a sequence. This process is based on the foundational concept of Attention from the paper Attention is All You Need, which allows models to process tokens and predict subsequent text through transformers. Without specific guidance, a model will choose the most statistically likely word based on its general training, which leads to inaccurate results in a specialized business setting.

The ERP Challenge: Why General AI Hallucinates

A standard LLM often provides incorrect but polite guesses, or hallucinations, when asked to write SQL for ERP systems because it lacks specific attention to your database structure. For example, a general model might try to select data from a table named customers, but in ERPNext, the correct table name is tabCustomer. Furthermore, models struggle with ambiguity; a user might ask about John or pens, but the system requires specific Customer Codes like T0034 or Item Codes like ITM-001 to execute a valid query. The problem is even greater with private apps and custom fields that are not published on GitHub, as the LLM has no way to guess these internal table and field names.

Reducing Hallucinations with RAG

To solve these issues and achieve accurate results, we implement Retrieval-Augmented Generation, or RAG. This architecture works in three specific stages:

Retrieval: The system retrieves specific facts from your local server, such as master table data, table names, field names, and the joins needed to connect them.

Augmentation: The original user question is enhanced or augmented with this retrieved metadata to provide the necessary context.

Generation: The LLM generates a precise response or SQL query grounded in these actual system facts rather than general probability.

The Role of Embedding Models

We use embedding models like Nomic-AI to convert words and schema into vectors, which are multi-dimensional numbers. These models allow the system to perform mathematical matching to find the nearest neighbor to a user’s query among billions of parameters. By supplying the right context as vectors, we shift the mathematical weight so the model generates the correct business logic instead of a generic answer.

Why RAG Focuses on Schema over Transactions

While it might seem helpful to send millions of sales invoices as context, transaction data is far too massive for an LLM to process efficiently. Instead, we use RAG to provide the map—specifically master tables, field names, and joins—and then rely on SQL or ORM to calculate totals and sums from the actual records. This ensures the AI provides the correct query to handle the heavy data processing.

Key Topics Covered:

The fundamental logic of Transformers and Attention.

Why generic LLMs fail with private ERPNext apps and custom fields.

Using RAG to ground AI in master table data and specific schema.

The difference between processing transactions and providing context.

How embedding models like Nomic-AI manage mathematical vectors.

#ERPNext

#AI

#RAG

#Qwen

#MachineLearning

#SQL

#OpenSource

#DataScience

#Python

#softwareengineering

#erp

Discussion about this video

Comments

Restacks

Technical podcast from ERPGulf team. Generally covers Frappe, ERPNext, E-Invoicing, AI , RAG

Claudion hosting ERPNext servers in Saudi Arabia

Claudion hosting ERPNext servers in Saudi Arabia

Subscribe

Listen on

Substack App

Spotify

RSS Feed

Appears in episode

Claudion

Recent Episodes

Episode 3: Technical Implementation—Fine-Tuning, Enriched Schemas, and Deployment

Jan 18

•

Claudion

Building an AI Agent for ERPNext - By ERPGulf - Episode-1

Jan 10

•

Claudion

Advance Invoices for ERPGulf ZATCA module.

Dec 11, 2025

•

Claudion

Claudion - Introducing Snapshots: Backup and Restore Your Instance Easily

Jul 14, 2025

•

Claudion

ZATCA Intermediary Server for Legacy Systems, SAP, Oracle, etc | Built on ERPNext

May 11, 2025

•

Claudion

Mastering Notifications in ERPNext: Stay Alert, Stay Ahead

May 5, 2025

•

Claudion

Secure Your ERPNext Account with 2FA

Apr 28, 2025

•

Claudion

---

## ✓ What is RAG (retrieval augmented generation) | McKinsey

**URL:** https://www.mckinsey.com/featured-insights/mckinsey-explainers/what-is-retrieval-augmented-generation-rag?utm_source=chatgpt.com

What is retrieval-augmented generation (RAG)?

October 30, 2024

| Article

Retrieval-augmented generation, or RAG, is a process applied to large language models to make their outputs more relevant for the end user.

A golden outline of a speech bubble is filled with a jumble of colorful, balloon-like spheres. The spheres vary in size, color, and texture, and the image is set against a light-gray background.

(5 pages)

In recent years

, large language models (LLMs) have made tremendous progress in their ability to generate content. But some leaders who hoped these models would increase business efficiency and productivity have been disappointed. Off-the-shelf generative AI (gen AI) tools have yet to live up to the considerable hype surrounding them. Why is that? For one thing, LLMs are trained on only the information thatâs available to the providers that build them. This can limit their utility in environments where a wider range of more nuanced, enterprise-specific knowledge is needed.

Get to know and directly engage with senior McKinsey experts on RAG.

Lareina Yee

is a senior partner in McKinsey’s Bay Area office, where

Michael Chui

is a senior fellow and

Roger Roberts

is a partner;

Mara Pometti

is a consultant in the London office;

Patrick Wollner

is a consultant in the Vienna office; and

Stephen Xu

is a senior director of product management in the Toronto office.

Retrieval-augmented generation, or RAG, is a process applied to LLMs to make their outputs more relevant in specific contexts. RAG allows LLMs to access and reference information outside the LLMs own training data, such as an organizationâs specific knowledge base, before generating a responseâand, crucially, with citations included. This capability enables LLMs to produce highly specific outputs without extensive fine-tuning or training, delivering some of the benefits of a custom LLM at considerably less expense.

Consider a typical gen AI chatbot thatâs deployed in a customer service context. While it may offer some general guidance, because the chatbot is working from an LLM that was trained on only a specific amount of information, itâs therefore not accessing the enterpriseâs unique policies, procedures, data, or knowledge base. As a result, its answers will lack specificity and relevance to a userâs inquiry. For example, when a customer asks about the status of their account or payment options, the chatbot might respond with only generic information; because the chatbot isnât accessing the companyâs specific data, the response it gives doesnât consider that customerâs specific situation.

Explore the full McKinsey Explainers series

Because RAG deployments have access to vast amounts of information that is more up to date and enterprise-specific, they can provide much more accurate, relevant, and coherent outputs. This is particularly helpful in applications and use cases that require highly accurate outputs, such as enterprise knowledge management and copilots that are specific to a given

domain

(for example, a workflow or process, journey, or function within the company).

Learn more about

QuantumBlack, AI by McKinsey

.

How does RAG work?

RAG involves two phases: ingestion and retrieval. To understand these concepts, it helps to imagine a large library with millions of books.

The initial âingestionâ phase is akin to stocking the shelves and creating an index of their contents, which allows a librarian to quickly locate any book in the libraryâs collection. As part of this process, a set of dense vector representationsânumerical representations of data, also known as âembeddingsâ (for more, see sidebar, âWhat are embeddings?â)âis generated for each book, chapter, or even selected paragraphs.

What are embeddings?

Embeddings are numerical representations of words or phrases that are unique points in a multidimensional digital space, where similar ideas and concepts are clustered together. Each embedding is defined by a vectorâthat is, a set of numbers that describes a particular characteristic or trait of the word or phrase, such as color, shape, or meaning. A vector is a coordinate on a map: it pinpoints the exact location of something in relation to its other features. Embeddings allow LLMs to retrieve only the most relevant data. Just as a catalog system in a library allows a librarian to quickly locate related text, embeddings help users organize and retrieve relevant information. Hereâs an example of how they work:

The word âkingâ is represented as a vector in the multidimensional space.

The word âmanâ is also represented as a vector in that space. Because âkingâ and âmanâ share a semantic meaning, their vectors are similar as well.

The word âwoman,â on the other hand, is represented as a vector that is different from both âkingâ and âman.â

When we subtract âmanâ from âkingâ and add âwomanâ to the space, the vectors are m

[Content truncated...]

---

## ✗ https://www.sciencedirect.com/science/article/pii/S0957417426000746

**URL:** https://www.sciencedirect.com/science/article/pii/S0957417426000746

Error scraping: 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/pii/S0957417426000746

---

## ✗ https://www.quora.com/Can-you-provide-examples-of-how-RAG-models-retrieval-based-generative-models-are-used-in-real-world-applications

**URL:** https://www.quora.com/Can-you-provide-examples-of-how-RAG-models-retrieval-based-generative-models-are-used-in-real-world-applications

Error scraping: 403 Client Error: Forbidden for url: https://www.quora.com/Can-you-provide-examples-of-how-RAG-models-retrieval-based-generative-models-are-used-in-real-world-applications

---

